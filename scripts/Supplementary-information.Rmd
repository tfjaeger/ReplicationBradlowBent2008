---
title: "Supplementary information for Xie et al. Cross-talker generalization in foreign-accented speech perception"
author: "Florian Jaeger, Linda Liu, Xin Xie"
date: \today
geometry: margin=2cm
header-includes:
  - \usepackage{booktabs}
  - \usepackage{siunitx}
  - \usepackage{tabto}
  - \usepackage{soul}
  - \usepackage{xcolor}
  - \usepackage{placeins}
  - \usepackage{lscape}
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}
  - \makeatletter\renewcommand{\fps@table}{!ht}\makeatother
  - \setstcolor{red}
  - \usepackage{sectsty}
  - \sectionfont{\color{blue}} 
  - \subsectionfont{\color{blue}}
  - \subsubsectionfont{\color{darkgray}}
output:
  pdf_document: 
    fig_caption: yes
    fig_width: 7
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 4
  fontsize: 10pt
---



```{r set-options, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE}
library(knitr)

opts_chunk$set(dev = 'pdf',
               comment="", 
               echo=FALSE, warning=FALSE, message=FALSE,
               cache=TRUE, 
               size="footnotesize",
               tidy.opts = list(width.cutoff = 200),
               fig.width = 8, fig.height = 4.5, fig.align = "center")

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

color_block = function(color) {
  function(x, options) sprintf('\\color{%s}\\begin{verbatim}%s\\end{verbatim}\\color{black}',
                               color, x)
}
knitr::knit_hooks$set(error = color_block('red'))
knitr::knit_hooks$set(warning = color_block('orange'))
```


# TO DO
 + FILL IN DETAILS ABOUT HOW SIMILARITY MEASURES WERE OBTAINED.

```{r functions, echo=FALSE}
library(tidyverse)
library(magrittr)
library(modelr)
library(broom)
library(cowplot)
library(ggridges)
library(scales)
library(viridis)
library(lme4)
library(brms)
library(bridgesampling)
library(tidybayes)
library(bayesplot)
library(actuar)
library(fitdistrplus)
library(pBrackets)
library(lubridate)

## ------------------------------ ##
## functions and constants 
## ------------------------------ ##
nsamples.total.max = 11000

path <- ""
max.cores = min(parallel::detectCores(), 8)
options(mc.cores = max.cores, width = 1000)
source(paste0(path, "../scripts/functions.R"))

dotplot_binwidth = 0.02
levels.exposure = c("Control", "Single talker", "Multi-talker", "Talker-specific")
colors.exposure = c("#B1B1B1", "#1F78B4", "#33A02C", "#E31A1C")
colors.model = viridis::viridis_pal()(3)  # without by-talker effects, by test talker, by exposure-test talker combinations


my_scale = function(x) (x - mean(x)) / (2 * sd(x, na.rm = T))

standardize = function(dataset) {
  dataset %>%
    mutate_at(vars(starts_with("individual_training_performance")),
      # Standardize continuous predictor (following Gelman, 2008)
      .funs = list("s" =  my_scale)
    )
}


defineContrasts = function(dataset) {
  require(dplyr)
  require(magrittr)
  
  dataset %<>%
    mutate(
      # For the sake of continuity (previously run models), reverse factor order here
      # so that coding stays consistent across models and data. Note that this means 
      # that the ordering of Condition2 (used in e.g., empirical plots) is the reverse
      # of the order of Cond.treat and Cond.diff
      Cond.treat = factor(Condition2, levels = rev(levels.exposure)),
      Cond.treatTS.vs.CNTL = case_when(
        Cond.treat == "Talker-specific" ~ 1,
        Cond.treat == "Multi-talker" ~ 0,
        Cond.treat == "Single talker" ~ 0,
        T ~ 0
      ),
      Cond.treatMT.vs.CNTL = case_when(
        Cond.treat == "Talker-specific" ~ 0,
        Cond.treat == "Multi-talker" ~ 1,
        Cond.treat == "Single talker" ~ 0,
        T ~ 0
      ),
      Cond.treatST.vs.CNTL = case_when(
        Cond.treat == "Talker-specific" ~ 0,
        Cond.treat == "Multi-talker" ~ 0,
        Cond.treat == "Single talker" ~ 1,
        T ~ 0
      ),
      # This numerical coding is used to address question 3 through treatment coding in the
      # replication analysis
      Cond.treatTS.vs.MT = case_when(
        Cond.treat == "Talker-specific" ~ 1,
        Cond.treat == "Multi-talker" ~ 0,
        Cond.treat == "Single talker" ~ 0,
        T ~ 0
      ),
      Cond.treatST.vs.MT = case_when(
        Cond.treat == "Talker-specific" ~ 0,
        Cond.treat == "Multi-talker" ~ 0,
        Cond.treat == "Single talker" ~ 1,
        T ~ 0
      ),
      Cond.treatCNTL.vs.MT = case_when(
        Cond.treat == "Talker-specific" ~ 0,
        Cond.treat == "Multi-talker" ~ 0,
        Cond.treat == "Single talker" ~ 0,
        T ~ 1
      ),
      # For the sake of continuity (previously run models), reverse factor order here
      Cond.diff = factor(Condition2, levels = rev(levels.exposure)),
      Cond.diffTS.vs.MT = case_when(
        Cond.diff == "Talker-specific" ~ 3/4,
        Cond.diff == "Multi-talker" ~ -1/4,
        Cond.diff == "Single talker" ~ -1/4,
        T ~ -1/4
      ),
      Cond.diffMT.vs.ST = case_when(
        Cond.diff == "Talker-specific" ~ 1/2,
        Cond.diff == "Multi-talker" ~ 1/2,
        Cond.diff == "Single talker" ~ -1/2,
        T ~ -1/2
      ),
      Cond.diffST.vs.CNTL = case_when(
        Cond.diff == "Talker-specific" ~ 1/4,
        Cond.diff == "Multi-talker" ~ 1/4,
        Cond.diff == "Single talker" ~ 1/4,
        T ~ -3/4
      )
    )

  # set sliding treaterence coding
  contrasts(dataset$Cond.treat) = cbind("TS.vs.CNTL" = c(1, 0, 0, 0),
                                        "MT.vs.CNTL" = c(0, 1, 0, 0),
                                        "ST.vs.CNTL" = c(0, 0, 1, 0))
  # set sliding difference coding
  contrasts(dataset$Cond.diff) = cbind("TS.vs.MT" = c(3/4, -1/4, -1/4, -1/4),
                                       "MT.vs.ST" = c(1/2, 1/2, -1/2, -1/2),
                                       "ST.vs.CNTL" = c(1/4, 1/4, 1/4, -3/4))
  
  return(dataset) 
}


table_3effects = function(model, 
                          scope = c("standard", "coef", "ranef")[1],
                          group = c("", "TestTalkerID", "TestTalkerID:TrainingTalkerID")[1], 
                          labels = NULL, format = "markdown",
                          ...
) {
  if (group == "") assertthat::assert_that(scope == "standard")
  if (group != "") assertthat::assert_that(scope %in% c("coef", "ranef"))
  
  xhypotheses(
    list(
      hypothesis(model, "Cond.diffTS.vs.MT + Cond.diffMT.vs.ST + Cond.diffST.vs.CNTL > 0", class = "b", scope = scope, group = group),
      hypothesis(model, "Cond.diffMT.vs.ST + Cond.diffST.vs.CNTL > 0", class = "b", scope = scope, group = group),
      hypothesis(model, "Cond.diffST.vs.CNTL > 0", class = "b", scope = scope, group = group),
      hypothesis(model, "Cond.diffMT.vs.ST > 0", class = "b", scope = scope, group = group)
    ),
    labels = rep(
      c("Adaptation: TS vs. CNTL", "Question 1: MT vs. CNTL", "Question 2: ST vs. CNTL", "Question 3: MT vs. ST"),
      case_when(
        scope == "standard" ~ 1L,
        T ~ length(grep(pattern = paste0("^r_", group, ".*Intercept"), x = parnames(model))))),
    ...
)
}


plot_3effects = function(
  model, 
  title, 
  add_plot = NULL, 
  color = "black", fill = "lightgray", alpha = .5,
  by = c("", "TestTalker", "ExposureTestTalker")[1],
  .width = c(.5, .95)
) {
  # Names of effects (ordered in the way they are meant to be displayed later)
  effect.names = c("Cond.MT.vs.ST", "Cond.ST.vs.CNTL", "Cond.MT.vs.CNTL", "Cond.TS.vs.CNTL")

  d.m = model %>%
    gather_draws(b_Cond.diffST.vs.CNTL, b_Cond.diffMT.vs.ST, b_Cond.diffTS.vs.MT) %>%
    ungroup() %>%
    rename(condition = .variable) %>%
    mutate(condition = str_replace(condition, "b_", "")) %>%
    # If effects are to be conditioned on test and/or exposure talker, add in those 
    # effects.
    { 
      if (by %in% c("TestTalker", "ExposureTestTalker")) {
        left_join(
          .,
          model %>%
            spread_draws(r_TestTalkerID[TestTalker,condition])) %>%
          mutate(
            .value = .value + r_TestTalkerID,
            r_TestTalkerID = NULL
          )
      } else . 
    } %>% 
    # If effects are to be conditioned on combination of test and exposure talker, add 
    # in those effects.
    { 
      if (by == "ExposureTestTalker") {
        stop("By exposure talker is not implememnted.")
        left_join(
          .,
          # model %>%
          #   spread_draws(`r_TestTalkerID:TrainingTalkerID`[ExposureTestTalker,condition])) %>%
          # mutate(
          #   .value = .value + `r_TestTalkerID:TrainingTalkerID`,
          #   `r_TestTalkerID:TrainingTalkerID` = NULL
          # )
          model %>%
            spread_draws(`r_TrainingUnderTestTalkerID`[ExposureTestTalker,Intercept])) %>%
          mutate(
            .value = .value + `r_TrainingUnderTestTalkerID`,
            `r_TrainingUnderTestTalkerID` = NULL,
            Intercept = NULL
          ) 
      } else . 
    } %>%
    pivot_wider(
      names_from = condition,
      values_from = .value,
    ) %>%
    mutate(
      Cond.TS.vs.CNTL = Cond.diffST.vs.CNTL + Cond.diffMT.vs.ST + Cond.diffTS.vs.MT,
      Cond.MT.vs.CNTL = Cond.diffST.vs.CNTL + Cond.diffMT.vs.ST,
      Cond.ST.vs.CNTL = Cond.diffST.vs.CNTL,
      Cond.MT.vs.ST = Cond.diffMT.vs.ST
    ) %>%
    dplyr::select(-starts_with("Cond.diff")) %>%
    pivot_longer(
      starts_with("Cond."),
      names_to = "condition",
      values_to = ".value"
    ) %>%
    mutate(condition = factor(condition, 
                              levels = effect.names))
  
  # If not plot was provided, make one. Otherwise add to provided plot.
  if (is.null(add_plot)) {
    p = d.m %>%
      ggplot(aes(y = condition, x = .value)) +
      scale_x_continuous(
        expression(paste(hat(beta)," (in log-odds)"))
      ) +
      scale_y_discrete("",
                       breaks = effect.names,
                       labels = c(
                         expression(paste(bold("Question 3   "), "Multi-talker vs.\nSingle talker")),
                         expression(paste(bold("Question 2   "), "Single talker vs.\nControl")),
                         expression(paste(bold("Question 1   "), "Multi-talker vs.\nControl")),
                         expression(paste(bold("Adaptation   "), "Talker-specific vs.\nControl"))
                       )
      ) + 
      ggtitle(title) +
      coord_cartesian(
        xlim = c(-.3, 1.3),
        ylim = c(.99, 4.8)
      ) 
  } else {
    p = add_plot
  }
  
  p = p + 
    { if (!is.null(.width)) 
      geom_halfeyeh(
        data = d.m,
        color = color, alpha = alpha, fill = fill, scale = .8, .width = .width) else 
      stat_slabh(
        data = d.m,
        color = color, alpha = alpha, fill = fill, scale = .8) } + 
    { if (by == "TestTalker") facet_grid( . ~ TestTalker) } +
    { if (by == "ExposureTestTalker") facet_wrap(~ ExposureTestTalker) } +
    geom_vline(xintercept = 0, linetype = 2) +
    theme_bw() +
    theme(axis.text.y = element_text(hjust=0))
  
  return(p)
}


plot_3effects_var = function(
  models.diff, title = NULL, 
  fills = viridis::viridis_pal()(length(models.diff)), alphas = rep(.5, length(models.diff)),
  add_plot = T,
  rel_widths = c(.7, .3)
) {
  # Names of effects (ordered in the way they are meant to be displayed later)
  effect.names = rev(c("Cond.MT.vs.ST", "Cond.ST.vs.CNTL", "Cond.MT.vs.CNTL", "Cond.TS.vs.CNTL"))

  # Check whether model names match. If no models are provided, number models.
  if (is.null(names(models.diff))) {
    names(models.diff) = as.character(1:length(models.diff))
  } 
  
  p = models.diff %>%                    
    purrr::map(function(x) 
      gather_draws(x, b_Cond.diffST.vs.CNTL, b_Cond.diffMT.vs.ST, b_Cond.diffTS.vs.MT)) %>%
    bind_rows(.id = "Model") %>%
    ungroup() %>%
    rename(condition = .variable) %>%
    mutate(condition = str_replace(condition, "b_", "")) %>%
    pivot_wider(
      names_from = condition,
      values_from = .value,
    ) %>%
    mutate(
      Cond.TS.vs.CNTL = Cond.diffST.vs.CNTL + Cond.diffMT.vs.ST + Cond.diffTS.vs.MT,
      Cond.MT.vs.CNTL = Cond.diffST.vs.CNTL + Cond.diffMT.vs.ST,
      Cond.ST.vs.CNTL = Cond.diffST.vs.CNTL,
      Cond.MT.vs.ST = Cond.diffMT.vs.ST
    ) %>%
    dplyr::select(-starts_with("Cond.diff")) %>%
    pivot_longer(
      starts_with("Cond."),
      names_to = "condition",
      values_to = ".value"
    ) %>%
    mutate(
      Model = factor(Model, levels = names(models.diff)),
      condition = factor(condition, 
                         levels = effect.names)) %>%
    group_by(Model, condition) %>%
    # Using SD of posterior samples to estimate SE of estimate.
    dplyr::summarise(.value = sd(.value)) %>%
    ggplot(aes(x = Model, y = .value, fill = Model)) +
    geom_bar(stat = "identity") +
    scale_x_discrete(name = NULL, labels = NULL) +
    scale_y_continuous("Standard error of effect") +
    scale_fill_manual(values = fills) +
    ggtitle(title) +
    facet_wrap(~ condition, ncol = 1) +
    theme_bw() + 
    theme(
      strip.background = element_blank(),
      strip.text.x = element_blank(),
      panel.grid = element_blank(),
      axis.text.x = element_text(angle = 22.5),
      legend.position = "right")
  
  if (!is.null(add_plot)) {
    p = cowplot::plot_grid(
      plotlist = list(add_plot, p), nrow = 1,
      rel_widths = rel_widths,
      align = "vh", axis = "tb"
    )
  }
  
  return(p)
}


plot_predicted_outcome = function(
  data,
  model,
  n.draws = 1000,
  experiment = c("1a", "1b"),
  groups = c("TestTalkerID", "TrainingUnderTestTalkerID"),
  offset.term = "individual_training_performance.offset"
) {
  d = data %>%
    group_by(WorkerID, TestTalkerID, TrainingTalkerID, TrainingUnderTestTalkerID, Condition2, Cond.diff, !! sym(offset.term), Sentence) %>%
    summarize(Correct = sum(IsCorrect) / length(IsCorrect)) %>%
    ungroup() %>%
    arrange(TestTalkerID, TrainingTalkerID, WorkerID, Sentence)
  
  m = d %>%
    add_fitted_draws(model, n = n.draws) %>%
    # Add offset term back in, so that we're showing fit while removing effects of offset
    mutate(.value = plogis(qlogis(.value) - !! sym(offset.term))) %>%
    group_by(!!! syms(groups), Condition2, .draw) %>%
    summarise(.value = mean(.value)) %>%
    mean_hdci()
    
  myGplot.defaults("paper")
  p = m %>%
    ggplot(aes(x = Condition2, color = Condition2, fill = Condition2, y = .value)) +
    # actual subject data
    geom_dotplot(
      binaxis = "y",stackdir = "center", width = .1, binwidth = .01, 
      data = d %>% 
        group_by(WorkerID, Condition2, !!! syms(groups)) %>%
        summarise(.value = mean(Correct)),
      alpha = .1
    ) +
    # prediction and 95% CI
    { if ("TrainingUnderTestTalkerID" %in% groups) 
        geom_pointrange(aes(ymax = .upper, ymin = .lower, group = TrainingUnderTestTalkerID),
                    position = position_dodge(.35)) else 
        geom_pointrange(aes(ymax = .upper, ymin = .lower)) } +
    xlab("Exposure condition") +
    scale_y_continuous("Proportion of keywords correctly transcribed") +
    scale_colour_manual("Exposure Condition", 
                        values = setNames(colors.exposure, levels.exposure), 
                        guide = F) +
    scale_fill_manual("Exposure Condition", 
                      values = setNames(colors.exposure, levels.exposure), 
                      guide = F) +
    coord_cartesian(ylim = c(0.75,1.0)) +
    { if (length(groups) > 0) facet_grid(cols = vars(!!! syms(groups[1]))) }  +
    theme(legend.position = "bottom", panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())
  
  ggsave(p, 
         filename = paste0("../figures/exp", experiment, "_predicted outcome_by", paste(groups, collapse = "+") ,".pdf"),
         height = 3.7, width = 15)
  
  return(p)
}

# note that units here are "npc", the only unit (besides physical units) that makes sense
# when annotating the plot panel in ggplot2 (since we have no access to 
# native units)
b1 <- bracketsGrob(0.33, 0.05, 0, 0.05, h=0.05, lwd=2, col="black")
b2 <- bracketsGrob(1, 0.05, 0.66, 0.05, h=0.05,  lwd=2, col="black")
```

```{r load data, echo = FALSE}
## ------------------------------ ##
## load data 
## ------------------------------ ##
d.full = readRDS(paste0(path, "../data/replication_data_all-February-2019.RDS"))

# in case any duplicate lines are in the file (there shouldn't be)
# We're also excluding the 34 subjects in Exp 1b (TS and CNTL) that had been excluded by Linda based on some criterion
# we haven't been able to recover (we are excluding them so as to not introduce new choices into our analysis).
d.full %<>% 
  distinct() %>%
  filter(!(WorkerID %in% c("2180f41a8725fe745fa12e517d4e3d85", "f3c7f9107260b28295a44ba112135086", "6711a123d308555813e8a1ce45297ac9","1f226fd6f8d8b9dd70f8d83eeb672b7c","6ed0824af9c7713a6bca62d7ed1b5e08", "d02fca1f212fe59588b0e67a6a57d582", "68d947314c3afdbeacc938e82fd4d69f", "b98be9d1f206a0dec29c5e6b86939e38", "e4f1027fa1140a91ff8040577a5fd780", "99b977df227b9a61fa9cf7e45403e6f3", "deca13132e8a8052d95914441bf3a728", "5381381c1ee08125e20660918b4a78b2", "9bd5109bcda73b775310c230af8d2718", "6e424818fb2e6bc8687e67f5128bea78", "a5e46b3a3dcba160b93403a4e157367a", "a7536ad08257c77bbe46231129a50794", "729b851a9644dcb5d2c79bca9d819422", "3d8bc6c8b092148daa39e0894c5f390a", "330891747efeb533e4283f9b4e1d8a3c", "d0cf4bf7f9a5478431ad28bab305f2e9", "4c6e2df9c69e761b76ad116ac804e8cd", "950331449cb75037d7ed3a9694b6b2e1", "1398467a82982f9950ffc46a0e179538", "1ba2ec8fec36eb6aea2486ab7d4c28b1", "240993087e1f551c412efa7fecfd8164", "fe6be26d6b036511b11c5908a1fe0370", "ba6b541b2d47757e02b0cf34ff0c4cd9", "b8f46c628ccb07d23cc7bbe26a01c918", "06c41dc0f05a4ab365a49b80429cdb57", "26d1bcf1b5fcf16d4687b711c24e9211", "0e8b277766f49367eb7fdbd55b53135e", "9f3fd867d4b01febd97869e2f4d6c4ad", "5b3d8d04f9987fea654ec4f9e83e7e97", "1c79496522986c45c96ec13b51783217")))

## ------------------------------ ##
## variable clean up
## ------------------------------ ##
d.full %<>% 
  mutate_at(
    c("Sentence", "WorkerID", "TestTalkerID", "PartOfExp", "PresentationBlock", "Condition", 
      "TrainingTestSet", "TestTalkerName", "SentenceID",
      "TrainingTalkerID", "CurrentTalkerID", "ExpRep_subj", "Exp1_subj", "Exp2_subj", "Exp3_subj"),
    as.factor
  ) %>%
  mutate(
    TalkerID.gen = paste("Test talker", as.character(TestTalkerID)),
    Condition2 = factor(Condition2, levels = levels.exposure)
  )
```

```{r, new transcription coding, echo = FALSE}
# Copy column over so we still have a column that will have all the keywords together
d.full %<>%
  mutate(Keyword = Keywords) %>%
  tidyr::separate_rows(Keyword, sep = ",") %>%
  mutate(
    Keyword = trimws(Keyword),
    TranscriptionStripped = gsub("[[:punct:]]", "", tolower(as.character(Transcription))),
    IsCorrect =  as.integer(!is.na(mapply(match,
                                          Keyword,
                                          strsplit(TranscriptionStripped, ' '))))
  )

# Update Propkeywords correct
correct.keywords = d.full %>%
  group_by(WorkerID, Sentence) %>%
  summarise(NumKeywordsCorrect = sum(IsCorrect))

d.full %<>% 
  left_join(correct.keywords) %>%
  mutate(PropKeywordsCorrect = NumKeywordsCorrect / NumKeywords)
```

```{r, subsetting of data, echo=F}
## ------------------------------ ##
# Define subsets in order to specify contrast coding for factors with appropriate leveling
# Note: The Exp1_subj variables already take into account rejections
## ------------------------------ ##

# Experiment pilot (formerly 1: ts vs cntl original)
d.pilot <- d.full %>%
  filter(Exp1_subj == "yes") %>%
  droplevels(.) %>%
  mutate(Experiment = "Pilot")

# Apply this pipe to Exp 1a and 1b
prepare_talkerIDs = . %>%
  # Repair exposure talker ID
  arrange(Experiment, Condition2, TestTalkerID, TrainingTestSet, ListNum, WorkerID, PartOfExp, Trial) %>%
  # Get the unique order of exposure talkers (for this, we group by part of experiment)
  group_by(Experiment, Condition2, TestTalkerID, TrainingTestSet, ListNum, WorkerID, PartOfExp) %>%
  mutate(TrainingTalkerID = paste(unique(CurrentTalkerID), collapse = "__")) %>%
  # Now make sure that this information is shared at the worker ID level (so that it will
  # be available for the test trials)
  group_by(Experiment, Condition2, TrainingTestSet, ListNum, WorkerID) %>%
  # The following works only because we've sorted (a.o. things) by PartOfExp above, which means 
  # that test trials come before training trials
  mutate(
    TrainingTalkerID = last(TrainingTalkerID),
    TestTalkerID = first(CurrentTalkerID),
     # Set training talker ID to 0 if the condition is no single talker
    TrainingUnderTestTalkerID = case_when(
      Condition2 == "Single talker" ~ paste(TrainingTalkerID, TestTalkerID, sep = "__"),
      T ~ "0"
    ),
    FirstTrainingTalkerID = gsub("^([A-Z]+\\_[A-Z]+\\_[0-9]+)__.*$", "\\1", TrainingTalkerID)) %>%
  ungroup() %>%
  mutate_at(c("TrainingTalkerID", "TrainingUnderTestTalkerID", "TestTalkerID", "FirstTrainingTalkerID"), factor) %>%
  droplevels()

# Experiment 1a (formerly 2a): ts, cntl, st, mt
d.exp1a <- d.full %>%
  filter(Exp2_subj == "yes") %>%
  droplevels(.) %>%
  mutate(Experiment = "1a") %>%
  prepare_talkerIDs()

### Experiment 1b (formerly 2b or Replication: Full rep of Exp2)
d.exp1b <- d.full %>%
  filter(ExpRep_subj == "yes") %>%
  droplevels(.) %>%
  mutate(Experiment = "1b") %>%
  prepare_talkerIDs()

# Keep only distinct workers in the combined data frame.
d.exp1ab = rbind(d.exp1a, d.exp1b) %>%
  group_by(WorkerID, Sentence) %>%
  distinct(.keep_all = TRUE)
```

# README FIRST
This markdown document presents all analyses reported in the paper, and provides additional tables, visualization, and analyses. We have intentionally *not* set a random seed for our analyses. Thus, the specific numbers for the Bayesian models might differ from those reported in the paper. Over the course of our analysis, we conducted all analyses repeatedly and never noticed changes in the qualitative results.

**Note that the analyses in this markdown document might take several days to complete.**

We begin by reporting basic information about participants and item balancing, demographics and audio equipment. We then present analyses of the training and test data, including full reports of all models referred to in the main text of the paper, as well as auxiliary analyses that support the conclusions offered in the main text.

# Participant and item counts by conditions

## Participants across experiments (after exclusions)

### Pilot experiment
```{r number of subjects, echo=F}
d.pilot %>% 
  filter(PartOfExp == "test") %>%
  group_by(Condition2, TestTalkerID, Experiment) %>% 
  distinct(WorkerID) %>%
  xtabs(~TestTalkerID + Condition2 + Experiment, .)
```

### Experiments 1a and 1b

Level at which had to balance both experiments (unique combinations of exposure condition and test talker):
```{r}
d.exp1ab %>% 
  filter(PartOfExp == "test") %>%
  group_by(Condition2, TestTalkerName, Experiment) %>% 
  distinct(WorkerID) %>%
  xtabs(~TestTalkerName + Condition2 + Experiment, .)
```

All unique combinations of exposure condition, exposure talker order, and test talker (i.e., all experimental lists). The order of exposure talkers (only relevant in the control and multi-talker conditions) was randomized. 

Experiment 1a:

```{r}
d.exp1a %>%
  dplyr::select(Condition2, TestTalkerName, TrainingTalkerID, WorkerID) %>%
  distinct() %>%
  xtabs(~ TrainingTalkerID + TestTalkerName + Condition2, data = .)
```

Experiment 1b:

```{r}
d.exp1b %>%
  dplyr::select(Condition2, TestTalkerName, TrainingTalkerID, WorkerID) %>%
  distinct() %>%
  xtabs(~ TrainingTalkerID + TestTalkerName + Condition2, data = .)
```

## Keywords per participants across experiments (after exclusions)

```{r keywords per subject, echo=F}
d.pilot %>% 
  group_by(Experiment, Condition2, TrainingTestSet, WorkerID, PartOfExp) %>% 
  summarise(NumOfKeywords = length(WorkerID)) %>%
  group_by(Experiment, TrainingTestSet, Condition2, PartOfExp) %>% 
  summarise(MeanNumOfKeywords = mean(NumOfKeywords))

d.exp1ab %>% 
  group_by(Experiment, Condition2, TrainingTestSet, WorkerID, PartOfExp) %>% 
  summarise(NumOfKeywords = length(WorkerID)) %>%
  group_by(Experiment, TrainingTestSet, Condition2, PartOfExp) %>% 
  summarise(MeanNumOfKeywords = mean(NumOfKeywords))
```

## Demographics of participants

```{r demographic information}
myXtabs = function (formula, data) { print(round(prop.table(xtabs(formula = formula, data = data), margin = 2), 2)); cat("\n\n") }
# Participant gender distribution
d.exp1ab %>% 
  filter(PartOfExp == "test") %>%
  group_by(Condition2, TestTalkerID, Experiment, 
           Answer.rsrb.age, Answer.rsrb.race, Answer.rsrb.sex, Answer.rsrb.raceother, Answer.rsrb.ethnicity) %>% 
  distinct(WorkerID) %T>%
  myXtabs( ~ Answer.rsrb.sex + Experiment, .) %T>%
  myXtabs( ~ Answer.rsrb.race + Experiment, .) %T>%
  myXtabs( ~ Answer.rsrb.ethnicity + Experiment, .) %>%
  group_by(Experiment) %>% 
  dplyr::summarise(
    meanAge = mean(Answer.rsrb.age, na.rm = T),
    sdAge = sd(Answer.rsrb.age, na.rm = T),
    declined = length(which(is.na(Answer.rsrb.age))) / length(Answer.rsrb.age)
  )
```

## Audio equipment

```{r audio information}
d.exp1ab %>%
  group_by(Condition2, Experiment) %>% 
  distinct(WorkerID, .keep_all = TRUE) %>%
  xtabs(~ Condition2 + AudioType2_WoreHeadphones + Experiment, .)
#xtabs(~ TalkerID + Condition2 + TestTalkerID + ListNum + TrainingTestSet,.) #To find what to balance
```






# Performance during exposure phase in Experiments 1a and 1b

Recall that exposure consisted of 80 recordings total of 16 different sentence stimuli (from 1 to 5 talkers, depending on the exposure condition). Unbeknownst to participants, these 80 recordings were presented as 5 bins of 16 sentences. The order of the 5 bins was determined by the list. The following figure shows the transcription performance during exposure by condition, binned into the the five trial blocks.

```{r}
select_training = . %>%
  filter(PartOfExp == "training") %>%
  defineContrasts() %>%
  mutate(
    Quintile = as.numeric(cut(Trial, breaks = 5, labels = seq(1,5))) - 1,
    sQuintile = Quintile / 2*sd(Quintile))

select_test = . %>%
  filter(PartOfExp == "test") %>%
  defineContrasts()
  
# Split training and test data for Experiments 1a and 1b
d.exp1a.training = d.exp1a %>% select_training()
d.exp1b.training = d.exp1b %>% select_training()
d.exp1ab.training = 
  rbind(d.exp1a.training,
        d.exp1b.training) %>%
  defineContrasts() %>%
  mutate(Experiment = factor(Experiment, levels = c("1a", "1b")))
contrasts(d.exp1ab.training$Experiment) = cbind("1a vs. 1b" = c(.5,-.5))

d.exp1a.test = d.exp1a %>% select_test()
d.exp1b.test = d.exp1b %>% select_test()
```

Overall performance during exposure was rather comparable in Experiments 1a and 1b, as shown in the following figure. However, performance during the earliest exposure blocks was higher in Experiment 1b, compared to 1a, for the multi-talker and control conditions. What these two these two exposure conditions have in common is that they involve five exposure talkers, rather than one (for the same total number of stimuli), and thus more heterogenous stimuli. Performance was lower in Experiment 1b, compared to 1a, in the other two conditions, though these differences were smaller compared to the relative increase in performance in the control and multi-talker condition.

```{r, comparison exp1a exp1b}
myGplot.defaults(type = "paper")
d.exp1ab.training %>%
  group_by(WorkerID, Sentence) %>%
  distinct(.keep_all = TRUE) %>%
  group_by(Experiment, WorkerID, Condition2, Quintile) %>%
	dplyr::summarise(PropKeywordsCorrect = mean(PropKeywordsCorrect)) %>%
	ggplot(aes(x = Quintile + 1, y = PropKeywordsCorrect, colour = Condition2, fill = Condition2, shape = Experiment)) +
  	stat_summary(fun.data = mean_cl_boot, geom = "pointrange", position = position_dodge(.5))+ 
    xlab("Trial block during exposure") +
		scale_y_continuous("Proportion of keywords correctly transcribed") +
    scale_colour_manual("Exposure Condition", 
                        values = setNames(colors.exposure, levels.exposure), 
                        guide = F) +
    scale_fill_manual("Exposure Condition", 
                      values = setNames(colors.exposure, 
                                        levels.exposure), 
                      guide = F) +
    coord_cartesian(ylim = c(0.75,1.0)) +
    facet_grid(. ~ Condition2) +
    theme(legend.position = "bottom", panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())

ggsave(filename = "../figures/p.exp1ab.training.pdf", device = cairo_pdf, width = 6, height = 4.2, dpi=300)
```

Two results visible in the figure deserve discussion. First, the multi-talker condition exhibits substantially higher ceiling performance at the end of exposure, compared to the single talker and talker-specific conditions. Improvement was also {\em faster} in the multi-talker condition, compared to the single talker and talker-specific conditions (as confirmed by the Bayesian regression presented below). At first blush this might seem surprising---participants in the multi-talker condition have to categorize speech from five \textit{different} talkers, rather than just one talker. It is possible that the difference in performance between single and multi-talker exposure might point to the deployment of different learning mechanisms. An alternative explanation does, however, seem more plausible: in the single talker (and talker-specific) condition, participants hear the exact {\em same} 16 recordings five times each (the five trial blocks); in the multi-talker condition, participants hear the same 16 sentences five time each, but each of those five instances corresponds to a different \textit{recording from a different talker}. This means that a difficult-to-understand recording in the single talker condition will affect participants' performance \textit{five} times, whereas it affects participants' performance in the multi-talker condition only once. Given that there are only 16 unique sentence items, correct recognition of a sentence item likely provides a substantial boost in performance on that sentence item in subsequent trial blocks (dwarfing the disadvantage that results from the fact that the later presentations are recordings from other talkers). If this reasoning is along the right lines, it raises an interesting question for future work: would performance during single talker exposure (and perhaps even cross-talker generalization following single talker exposure) be facilitated if listeners are exposed to multiple \textit{different} recordings of the same sentence from the same talker? 

Second, the performance at the beginning of exposure was higher in Experiment 1b, compared to 1a, for both the multi-talker and control condition. The opposite trend held---though less clearly---for the two exposure conditions. It is possible that these differences reflect difference in the audio equipment between Experiments 1a and 1b. In particular, the proportion of participants who used headsets remained identical betwween Experiments 1a and 1b only for the talker-specific condition, but increased for the control and multi-talker conditions---i.e. precisely those conditions for which performance in Experiment 1b seems higher (during the earliest exposure trials), compared to Experiment 1a. Alternatively (or additionally), it is possible that the differences between the two experiments reflect changes in the population that we are recruiting from (about three years passed between recruitment for Experiment 1a and 1b). This possibility is explores in the section on *Performance across time*.

To further analyze these differences, we fit a Bayesian mixed-effects logistic regression to the combined exposure data from both experiments, predicting transcription accuracy during exposure as a function of experiment (sum-coded: -1 = Experiment 1a vs. 1 = Experiment 1b), the exposure condition (sliding difference-coded comparing the talker-specific condition against the multi-talker condition, the multi-talker condition against the single talker condition, and the single talker condition against the control), the trial block (coded from 0 to 4 for the five blocks of 16 sentence recordings during exposure), and their interactions. Additionally, the model contained random intercepts by subject, item, and the current exposure talker, as well as random slopes for exposure condition by item, and random slopes for trial block by participant. The by-talker random intercept captures inter-talker differences in intelligibility, as the stimuli in the first trial block during exposure can come from different talkers, depending both on the condition and the specific list. 

The results of the exposure analysis are summarized and visualized below.

```{r, training models summary, fig.width=5}
my.priors = c(
    # Following Gelman et al (2008) and later revisions on wiki.
    # First parameter is DFs: smaller means fatter (less normal) tails. As DFs approach, distribution approaches normality
    # Second parameter is mu: should be 0 to be unbiased
    # Third parameter is sigma: larger means wider
    prior(student_t(3,0,2.5), class = b),
    # 5 is a common choice for logit models. Consider using a stronger prior if convergence is an issue
    prior(cauchy(0,5), class = sd), 
    # Uniform prior of correlations
    prior(lkj(1), class = cor)
)

m.brm.exp1ab.train.wExperiment = brm(IsCorrect ~ Cond.diff * sQuintile * Experiment +
                           (1 + Cond.diff | Sentence) + (1 + sQuintile | WorkerID) + (1 | CurrentTalkerID), 
                         data = d.exp1ab.training, 
                         family = bernoulli, cores = max.cores, chains = 4, 
                         warmup = 2000, iter = 4000, 
                         prior = my.priors, file = "../models/exp1ab.Training.wExperiment")
plot(conditional_effects(m.brm.exp1ab.train.wExperiment, effects = c("Cond.diff:Experiment", "sQuintile:Experiment"),
                         # set to NULL to add uncertainty about all random effects
                         re_formula = NA), ask = F)

s = summary(m.brm.exp1ab.train.wExperiment)$fixed
colnames(s) = c("Est.", "SE", "$CI_{lower}$", "$CI_{upper}$", "$\\hat{R}$", "Eff. samples (bulk)", "Eff. samples (tail)")
rownames(s) = c("Intercept (1st Block)", 
                "TS vs. MT (1st Block)", "MT vs. ST (1st Block)", "ST vs. CNTL (1st Block)", 
                "Quintile", "Experiment", 
                "Qu:TS vs. MT", "Qu:MT vs. ST", "Qu:ST vs. CNTL", 
                "Exp:TS vs. MT", "Exp:MT vs. ST", "Exp:ST vs. CNTL", 
                "Qu:Exp", 
                "Qu:Exp:TS vs. MT", "Qu:Exp:MT vs. ST", "Qu:Exp:ST vs. CNTL")
kable(s, digits = c(2, 3, 2, 2, 1, 2), 
      caption = "Training analysis of Experiments 1a and 1b combined")

xhypotheses(
  list(
    hypothesis(m.brm.exp1ab.train.wExperiment, 
               "Cond.diffTS.vs.MT + Cond.diffMT.vs.ST + Cond.diffST.vs.CNTL < 0", class = "b"),
    hypothesis(m.brm.exp1ab.train.wExperiment, 
               "Cond.diffMT.vs.ST + Cond.diffST.vs.CNTL < 0", class = "b"),
    hypothesis(m.brm.exp1ab.train.wExperiment, 
               "Cond.diffST.vs.CNTL < 0", class = "b"),
    hypothesis(m.brm.exp1ab.train.wExperiment, 
               "Cond.diffMT.vs.ST < 0", class = "b"),
    hypothesis(m.brm.exp1ab.train.wExperiment, 
               "Cond.diffTS.vs.MT:Experiment1avs.1b + Cond.diffMT.vs.ST:Experiment1avs.1b + Cond.diffST.vs.CNTL:Experiment1avs.1b > 0", class = "b"),
    hypothesis(m.brm.exp1ab.train.wExperiment, 
               "Cond.diffMT.vs.ST:Experiment1avs.1b + Cond.diffST.vs.CNTL:Experiment1avs.1b > 0", class = "b"),
    hypothesis(m.brm.exp1ab.train.wExperiment, 
               "Cond.diffST.vs.CNTL:Experiment1avs.1b > 0", class = "b"),
    hypothesis(m.brm.exp1ab.train.wExperiment, 
               "Cond.diffMT.vs.ST:Experiment1avs.1b < 0", class = "b")),
  labels = c("TS < CNTL (1st Block)", "MT < CNTL (1st Block)", "ST < CNTL (1st Block)", "MT < ST (1st Block)",
             "Exp 1a vs. 1b - TS > CNTL", "Exp 1a vs. 1b - MT > CNTL", "Exp 1a vs. 1b - ST > CNTL", "Exp 1a vs. 1b - MT < ST")
)
```


## Potential consequences of individual differences in *a priori* performance

Whatever the reason for these difference, it is possible that they confound the analyses of the test data. **The analyses of performance during the test phase therefore control for individual differences in performance during exposure**. Specifically, the analyses of the test data use an offset term that is based on the individual difference in performance at the onset of the exposure phase. Next, we describe how we estimated these individual differences.^[Additional analyses previously conducted for Experiment 1a (which had a more asymmetric distribution of audio equipment across exposure conditions, see above) found that results did not change if audio equipment was included as a control predictor in the analyses.]


## Estimating individual differences in performance at the onset of exposure

Our goal was to estimate each individual's performance at the onset of the experiment, before effects of exposure could cause additional individual differences. This, we hope, can capture differences in quality of the audio equipment, differences in the degree to which participants paid attention, etc. We thus fit a Bayesian mixed-effects logistic regression, predicting transcription accuracy during exposure as a function of the exposure condition (*control*, *single talker*, *multi-talker*, and *talker-specific*), the trial block (from 0 to 4 for the five blocks of 16 sentence recordings during exposure), and their interaction. Additionally, the model contained random intercepts by subject, item, and the current exposure talker, as well as random slopes for exposure condition by item, and random slopes for trial block by participant. Critically, the model did not contain a predictor for experiment since we are interested in capturing individual differences in participants' performance across the entire sample of participants in Experiments 1a and 1b.

In this model, the random intercept by participant provide an estimate of each participants' performance *relative to other participants in the same exposure group* (since exposure condition was included in the model) *during the first trial block* (since the first trial block is coded as 0). These random by-participant intercepts are then used as an offset in the analysis of the test data.

```{r, fig.width=5}
# Model without experiment as predictor
m.brm.exp1ab.train = brm(IsCorrect ~ Cond.diff * sQuintile +
                           (1 + Cond.diff | Sentence) + (1 + sQuintile | WorkerID) + (1 | CurrentTalkerID), 
                         data = d.exp1ab.training, 
                         family = bernoulli, cores = max.cores, chains = 4, 
                         warmup = 2000, iter = 4000, 
                         prior = my.priors, file = "../models/exp1ab.Training.diff")
plot(conditional_effects(m.brm.exp1ab.train, effects = c("Cond.diff", "Cond.diff:sQuintile"),
                         # set to NULL to add uncertainty about all random effects
                         re_formula = NA), ask = F)
```




## How do our modeling assumptions affect the estimates of individual differences?

```{r fit monotonic model}
m.brm.exp1ab.train.monotonic = brm(IsCorrect ~ Cond.diff * mo(Quintile) +
                           (1 + Cond.diff | Sentence) + (1 + mo(Quintile) | WorkerID) + (1 | CurrentTalkerID), 
                         data = d.exp1ab.training, 
                         family = bernoulli, cores = max.cores, chains = 4, 
                         warmup = 2000, iter = 4000, 
                         prior = my.priors, file = "../models/exp1ab.Training.diff.monotonic")
```

```{r add individual training performance to training and test data}
## Extract random by-subject intercepts from the various models
get_random_intercepts = function(model) {
  ranef(model)[["WorkerID"]][,,"Intercept"] %>%
    as_tibble(rownames = "WorkerID") %>%
    dplyr::select(WorkerID, Estimate)
}

d.training.individual_performance = 
  m.brm.exp1ab.train %>%
  get_random_intercepts() %>%
  rename(individual_training_performance.linear = Estimate) %>%
  left_join(
    m.brm.exp1ab.train.monotonic %>%
    get_random_intercepts() %>%
    rename(individual_training_performance.monotonic = Estimate)
  ) %>%
  left_join(
    m.brm.exp1ab.train.wExperiment %>%
    get_random_intercepts() %>%
    rename(individual_training_performance.linear.wExperiment = Estimate)
  ) 

# Join individual training performance back into data frames
add_individual_performance = . %>%
  left_join(d.training.individual_performance) %>%
  defineContrasts() %>% 
  standardize() %>%
  # Store offset term to avoid ambiguity
  mutate(individual_training_performance.offset = individual_training_performance.linear)
  # set to following if coef from training is to be considered in discounting individual differences:
  # mutate(individual_training_performance.offset = individual_training_performance.linear * 0.6365345)

d.exp1ab.training = 
  rbind(d.exp1a.training, d.exp1b.training) %>%
  add_individual_performance()
d.exp1a.training %<>% add_individual_performance()
d.exp1b.training %<>% add_individual_performance()

d.exp1ab.test = 
  rbind(d.exp1a.test, d.exp1b.test) %>%
  add_individual_performance()
d.exp1a.test %<>% add_individual_performance()
d.exp1b.test %<>% add_individual_performance()
```

### Pooling of data across experiments without controlling for effects of experiment

We first compared the estimates of individual differences against estimates derived from the full model that additionally contained experiment as a predictor, as well as its interaction with trial block and exposure condition. The goal of this comparison is to see how our decision to *not* include experiment in the estimation of individual differences affects the estimated individual differences. The black shapes show the mean across participants in that condition and experiment.

```{r, fig.height=6, fig.width=7}
d.exp1ab.test %>%
  dplyr::select(Experiment, WorkerID, Condition2, starts_with("individual_training_performance")) %>%
  distinct() %>%
  group_by(Experiment, WorkerID, Condition2) %>%
  dplyr::summarise_at(vars(starts_with("individual_training_performance")), mean) %>%
  { . ->> temp } %>%
ggplot(aes(x = individual_training_performance.linear, y = individual_training_performance.linear.wExperiment, shape = Experiment, color = Experiment)) +
  geom_abline(slope = 1) +
  geom_point(alpha = .5) +
  geom_point(data = temp %>%
         group_by(Experiment, Condition2) %>%
         dplyr::summarise_all(mean),
         size = 3, alpha = 1, color = "black"
  ) +
  scale_x_continuous("by-participant adjustment from linear model") +
  scale_y_continuous("by-participant adjustment from linear model + experiment") +
  facet_wrap(~ Condition2)
```

Zooming in ...

```{r}
last_plot() + coord_cartesian(xlim = c(-.5, .5), ylim = c(-.5, .5))
```

### Relaxing the assumption of linear effects during exposure 

Finally, the model we used above to estimate individual differences at the onset of exposure assumes that effect of increasing exposure (trial blocks) on the log-odds of accurate transcriptions are linear. To ascertain that this linearity assumption does not introduce undue bias to the estimation of the effects at the beginning of the experiment, we also fit a model that did only assume monotonicity of the changes across qunitiles, rather than the stronger qssumption of linearity (in log-odds). This model makes a very similar predictions for the effects during exposure:

```{r, fig.width=5}
plot(conditional_effects(m.brm.exp1ab.train.monotonic, 
                         # set to NULL to add uncertainty about all random effects
                         re_formula = NA), ask = F)
```

Critically, there is no evidence that the linearity assumption causes any bias in the estimation of individual differences, compared to the model that merely assumes monotonicity. The two models provide *very similar* estimates of participants' performance during the first exposure block:

```{r, fig.height=6, fig.width=7}
d.exp1ab.test %>%
  dplyr::select(Experiment, WorkerID, Condition2, starts_with("individual_training_performance")) %>%
  distinct() %>%
  group_by(Experiment, WorkerID, Condition2) %>%
  dplyr::summarise_at(vars(starts_with("individual_training_performance")), mean) %>%
  { . ->> temp } %>%
ggplot(aes(x = individual_training_performance.linear, y = individual_training_performance.monotonic, shape = Experiment, color = Experiment)) +
  geom_abline(slope = 1) +
  geom_point(alpha = .5) +
  geom_point(data = temp %>%
         group_by(Experiment, Condition2) %>%
         dplyr::summarise_all(mean),
         size = 3, alpha = 1, color = "black"
  ) +
  scale_x_continuous("by-participant adjustment from linear model") +
  scale_y_continuous("by-participant adjustment from monotonic model") +
  facet_wrap(~ Condition2)
```

Zooming in ...

```{r}
last_plot() + coord_cartesian(xlim = c(-.5, .5), ylim = c(-.5, .5))
```

In the remainder of the analyses, we thus used the linear model because it is more common and readers are more familiar with it. Given the high similarity in the estimates, we never investigated whether individual differences based on the monotonic model would change any of the results for the test data.

## Predicting individual difference during test from individual differences during training

In the analyses of the test data, we discount for the individual differences in performance at the onset of test. We do so by using the random by-participant intercept from the linear model as **offset** in the mixed-effects logistic regression of the test data. It was therefore important to test whether individual differences during exposure actually predict individual differences during test. To this end, we fit a Bayesian mixed-effect logistic regression, predicting accuracy during test as a function of the individual differences estimated from the linear model. The model also contained random intercepts by participant and item. This revealed a very strong effect of individual differences during exposure on performance during test.

```{r}
nsamples.warmup = 2000
nsamples.total = 4000

# Let's scale continuous variables to have SD .5 (i.e. divide through 2 SDs) and set mean to maximally 
# ambiguous continuum step (rather than center). Binary factors should be coded to have level-
# distance of 1. Then we follow https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations 
# and set the scale of the our t-prior to 2.5
my.priors = c(
    # Following Gelman et al (2008) and later revisions on wiki.
    # First parameter is DFs: smaller means fatter (less normal) tails. 
    # Second parameter is mu: should be 0 to be unbiased
    # Third parameter is sigma: larger means wider
    prior(student_t(3,0,2.5), class = b),
    # 5 is a common choice for logit models. Consider using a stronger prior if convergence is an issue
    prior(cauchy(0,5), class = sd)
    # , 
    # # Uniform prior of correlations
    # prior(lkj(1), class = cor)
)

m.exp1ab.test.fromTraining <- brm(
  IsCorrect ~ 
    1 + individual_training_performance.linear_s +
    (1 | Sentence) + (1 | WorkerID),  
  data = d.exp1ab.test %>% standardize(), 
  family = bernoulli, cores = 4, chains = 4, 
  warmup = nsamples.warmup, iter = nsamples.total, 
  prior = my.priors, file = "../models/exp1ab.Test.fromIndividualTrainingDifferences")
```

Separated by exposure condition, we see very little variability in the exact estimate of how much individual differences estimated during exposure affect performance during test. The estimated coefficients show a remarkable degree of agreement across the four (entirely non-overlapping) test data for the four different exposure conditions:

```{r}
m.exp1ab.test.fromTraining.onlyTS <- brm(
  IsCorrect ~ 
    1 + individual_training_performance.linear_s +
    (1 | Sentence) + (1 | WorkerID),  
  data = d.exp1ab.test %>% 
    filter(Cond.diff == "Talker-specific") %>%
    standardize(), 
  family = bernoulli, cores = 4, chains = 4, 
  warmup = nsamples.warmup, iter = nsamples.total, 
  prior = my.priors, file = "../models/exp1ab.Test.fromIndividualTrainingDifferences.onlyTS")

m.exp1ab.test.fromTraining.onlyMT <- brm(
  IsCorrect ~ 
    1 + individual_training_performance.linear_s +
    (1 | Sentence) + (1 | WorkerID),  
  data = d.exp1ab.test %>% 
    filter(Cond.diff == "Multi-talker") %>%
    standardize(), 
  family = bernoulli, cores = 4, chains = 4, 
  warmup = nsamples.warmup, iter = nsamples.total, 
  prior = my.priors, file = "../models/exp1ab.Test.fromIndividualTrainingDifferences.onlyMT")

m.exp1ab.test.fromTraining.onlyST <- brm(
  IsCorrect ~ 
    1 + individual_training_performance.linear_s +
    (1 | Sentence) + (1 | WorkerID),  
  data = d.exp1ab.test %>% 
    filter(Cond.diff == "Single talker") %>%
    standardize(), 
  family = bernoulli, cores = 4, chains = 4, 
  warmup = nsamples.warmup, iter = nsamples.total, 
  prior = my.priors, file = "../models/exp1ab.Test.fromIndividualTrainingDifferences.onlyST")

m.exp1ab.test.fromTraining.onlyCNTL <- brm(
  IsCorrect ~ 
    1 + individual_training_performance.linear_s +
    (1 | Sentence) + (1 | WorkerID),  
  data = d.exp1ab.test %>% 
    filter(Cond.diff == "Control") %>%
    standardize(), 
  family = bernoulli, cores = 4, chains = 4, 
  warmup = nsamples.warmup, iter = nsamples.total, 
  prior = my.priors, file = "../models/exp1ab.Test.fromIndividualTrainingDifferences.onlyCNTL")

xhypotheses(
  list(
    hypothesis(m.exp1ab.test.fromTraining.onlyTS, "individual_training_performance.linear_s > 0"),
    hypothesis(m.exp1ab.test.fromTraining.onlyMT, "individual_training_performance.linear_s > 0"),
    hypothesis(m.exp1ab.test.fromTraining.onlyST, "individual_training_performance.linear_s > 0"),
    hypothesis(m.exp1ab.test.fromTraining.onlyCNTL, "individual_training_performance.linear_s > 0")),
  labels = paste0("Individual training performance > 0 (", c("TS", "MT", "ST", "CNTL"), " condition)")
)
```
Te effect of the individual differences predictor can also be seen in the reduction of the variability in the by-participant random intercepts during test in the model that controlled for individual differences in performance---here shown for the combined data from all exposure conditions of both experiments (SD = .60):

```{r}
summary(m.exp1ab.test.fromTraining)
```

compared to a model that did not (SD = .74): 

```{r}
# Estimate the by-subject variance during test if performance at onset of training is *not* taken into account:
m.exp1ab.test.justIntercept <- brm(IsCorrect ~ 
                           1 + 
                           (1 | Sentence) + (1 | WorkerID),  
                         data = d.exp1ab.test %>% standardize(), 
                         family = bernoulli, cores = 4, chains = 4, 
                         warmup = nsamples.warmup, iter = nsamples.total, 
                         prior = my.priors[-1,], file = "../models/exp1ab.Test.justIntercept")
summary(m.exp1ab.test.justIntercept)
```

In other words, individual differences in performance during training relative to other participants that see the same exposure materials are indeed highly predictive of individual differences in performance during test. This highlights the potential that even small imbalanced in these individual differences across the four conditions in Experiments 1a and 1b could make the test results across the two experiments appear less similar than they are (once individual differences are taken into account).

If we translate the estimated coefficient back into the un-standardized individual differences at the onset of the exposure block (`individual_training_performance.linear`) this yields the following estimates for the relation of individual differences in performance at the onset of training and the performance during test for the four different exposure conditions:

```{r}
d.exp1ab.test %>%
  filter(Condition2 == "Control") %>%
  dplyr::select(individual_training_performance.linear) %>%
  summarise_at(vars(starts_with("individual_training_performance")), function(x) 2 * sd(x)) %>%
  as.numeric() %>%
  (function(x) summary(m.exp1ab.test.fromTraining.onlyCNTL)$fixed[2,c(1,3:4)] / x)

d.exp1ab.test %>%
  filter(Condition2 == "Single talker") %>%
  dplyr::select(individual_training_performance.linear) %>%
  summarise_at(vars(starts_with("individual_training_performance")), function(x) 2 * sd(x)) %>%
  as.numeric() %>%
  (function(x) summary(m.exp1ab.test.fromTraining.onlyST)$fixed[2,c(1,3:4)] / x)

d.exp1ab.test %>%
  filter(Condition2 == "Multi-talker") %>%
  dplyr::select(individual_training_performance.linear) %>%
  summarise_at(vars(starts_with("individual_training_performance")), function(x) 2 * sd(x)) %>%
  as.numeric() %>%
  (function(x) summary(m.exp1ab.test.fromTraining.onlyMT)$fixed[2,c(1,3:4)] / x)

d.exp1ab.test %>%
  filter(Condition2 == "Talker-specific") %>%
  dplyr::select(individual_training_performance.linear) %>%
  summarise_at(vars(starts_with("individual_training_performance")), function(x) 2 * sd(x)) %>%
  as.numeric() %>%
  (function(x) summary(m.exp1ab.test.fromTraining.onlyTS)$fixed[2,c(1,3:4)] / x)
```

In the analysis of the test data, we thus include the individual differences in performance at the onset of exposure. We include this variable, not as a predictor, but as an offset term. This sets the coefficient of this effect to 1, higher than the effect observed here. We did so based on the *a priori* consideration that the task during test is the same as during exposure so that any individual differences in the log-odds of accurate transcriptions during exposure (compared to other participants who heard the exact same stimuli) should translate to the exact same difference in log-odds of accurate transcriptions during test.



\newpage
# Test - Experiment 1a

In order to address the three questions we ask in the main text (Table 1), we need to compare test performance between the multi-talker and control condition (*Question 1: Is there cross-talker generalization after multi-talker exposure?*), between the single talker and control condition (*Question 2: Is there cross-talker generalization after single talker exposure?*), and the multi-talker and single talker condition (*Question 3: Does multi-talker exposure facilitate cross-talker generalization, compared to single talker exposure?*). 

To achieve these comparisons, we use `brm()` from the `brms` package to fit Bayesian mixed-effects logistic regression to the test data, predicting the accuracy of participants' responses from the four exposure conditions (control, single talker, multi-talker, talker-specific), and the control predictor for individual differences (see main text for details). The random effect structure was maximal, i.e., random intercepts by subject and item, as well as random slopes for the exposure condition by item (since exposure condition was manipulated between subjects, no by subject slopes were included).

All hypothesis tests reported in the paper are based on a model that uses sliding difference-coding for the the exposure conditions. The *sliding difference-coded model* imposes an order among the four conditions (control < single talker < multi-talker < talker-specific exposure), and compares each condition against the next 'lower' condition. The first contrast compares the single talker against the control condition. The second contrast compares the multi-talker against the single talker condition. The third contrast compares the talker-specific against the multi-talker condition. This coding has the advantage the resulting predictors are orthogonal to each other (unlike for, e.g., treatment-coding). The `hypothesis()` function of the `brms` package makes it easy to test hypotheses about sums of effects. For example, the Bayesian hypothesis test for our first question (*Is there cross-talker generalization after multi-talker exposure?*) asks whether the sum of the single talker vs. control effect (resulting from the first sliding difference-coded contrast) plus the multi-talker vs. single talker effect (resulting from the second sliding difference-coded contrast) is larger than zero. The test for our second question (*Is there cross-talker generalization after single talker exposure?*) asks whether the single talker vs. control effect by itself is larger than zero. The test for our third question (*Is there cross-talker generalization after single talker exposure?*) asks whether the multi-talker vs. single talker effect (resulting from the third sliding difference-coded contrast) is larger than zero. And, finally, the hypothesis test for the talker-specific effect---provided for comparison, as it is has been replicated many times across studies---ask whether the sum of all three sliding difference-coded contrasts is larger than zero.

Since some readers might be more familiar with treatment-coding, we also present a *treatment-coded model* that compares each condition against the control condition as baseline. The first contrast compares the talker-specific condition against the control. The second contrast compares the multi-talker condition against the control. The third contrast compares the single-talker condition against the control. The treatment-coded model differed from the sliding difference-coded model only in terms of the contrasts employed for the exposure conditions.

We first present the result summary for the three questions of interest. These are the results presented in the main text. Then we we present the complete model output and additional Bayesian hypothesis tests for both the difference- and treatment-coded models.

```{r, exp1a test block analysis - main level - also plot this}
nsamples.warmup = 1000
nsamples.total = nsamples.total.max

# Let's scale continuous variables to have SD .5 (i.e. divide through 2 SDs) and set mean to maximally 
# ambiguous continuum step (rather than center). Binary factors should be coded to have level-
# distance of 1. Then we follow https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations 
# and set the scale of the our t-prior to 2.5
my.priors = c(
    # Following Gelman et al (2008) and later revisions on wiki.
    # First parameter is DFs: smaller means fatter (less normal) tails. As DFs approach, distribution approaches normality
    # Second parameter is mu: should be 0 to be unbiased
    # Third parameter is sigma: larger means wider
    prior(student_t(3,0,2.5), class = b),
    # 5 is a common choice for logit models. Consider using a stronger prior if convergence is an issue
    prior(cauchy(0,5), class = sd), 
    # Uniform prior of correlations
    prior(lkj(1), class = cor)
)

# m.exp1a.test.treat <- brm(IsCorrect ~ Cond.treat + individual_training_performance.linear_s + 
#                             (1 + Cond.treat | Sentence) + (1 | WorkerID),  
#                           data = d.exp1a.test, family = bernoulli, cores = max.cores, chains = max.cores, 
#                           warmup = nsamples.warmup, iter = nsamples.total,
#                           prior = my.priors, file = "../models/exp1a.Test.treat")
# m.exp1a.test.diff <- brm(IsCorrect ~ Cond.diff + individual_training_performance.linear_s + 
#                            (1 + Cond.diff | Sentence) + (1 | WorkerID),  
#                          data = d.exp1a.test, family = bernoulli, cores = max.cores, chains = max.cores, 
#                          warmup = nsamples.warmup, iter = nsamples.total, 
#                          prior = my.priors, file = "../models/exp1a.Test.diff")

# offset model
m.exp1a.test.treat <- brm(
  IsCorrect ~ Cond.treat + offset(individual_training_performance.offset) +
    (1 + Cond.treat | Sentence) + (1 | WorkerID),
  data = d.exp1a.test, family = bernoulli, cores = max.cores, chains = max.cores,
  warmup = nsamples.warmup, iter = nsamples.total,
  prior = my.priors, file = "../models/exp1a.Test.treat")

m.exp1a.test.diff <- brm(
  IsCorrect ~ 
    offset(individual_training_performance.offset) +
    1 + Cond.diff + 
    (1 + Cond.diff | Sentence) + (1 | WorkerID),  
  data = d.exp1a.test, family = bernoulli, cores = max.cores, chains = max.cores, 
  warmup = nsamples.warmup, iter = nsamples.total, 
  prior = my.priors, file = "../models/exp1a.Test.diff")

# Without any correction for a priori performance
# m.exp1a.test.treat.woCorrection <- brm(IsCorrect ~ Cond.treat + 
#                                          (1 + Cond.treat | Sentence) + (1 | WorkerID),  
#                                        data = d.exp1a.test, family = bernoulli, cores = max.cores, chains = max.cores, 
#                                        warmup = nsamples.warmup, iter = nsamples.total, 
#                                        prior = my.priors, file = "../models/exp1a.Test.treat.woCorrection")
# m.exp1a.test.diff.woCorrection <- brm(IsCorrect ~ Cond.diff + 
#                                         (1 + Cond.diff | Sentence) + (1 | WorkerID),  
#                                       data = d.exp1a.test, family = bernoulli, cores = max.cores, chains = max.cores, 
#                                       warmup = nsamples.warmup, iter = nsamples.total, 
#                                       prior = my.priors, file = "../models/exp1a.Test.diff.woCorrection")

myGplot.defaults(type = "paper")
plot_base = 
  d.exp1a %>%
  group_by(WorkerID, Sentence) %>%
  distinct(.keep_all = TRUE) %>%
  filter(PartOfExp == "test") %>% 
  group_by(WorkerID, Condition2) %>%
  dplyr::summarise(PropKeywordsCorrect = mean(PropKeywordsCorrect)) %>%
  ggplot(aes(x = Condition2, y = PropKeywordsCorrect, colour = Condition2, fill = Condition2)) +
  stat_summary(fun = "mean", geom = "point", 
               alpha = 1, size = 1.1, stroke=3, position = position_dodge(.9)) +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", 
               width = 0.2, linetype = 1, size = 1, position = position_dodge(.9)) +
  scale_x_discrete("Exposure condition") +
  scale_colour_manual("Exposure Condition", values = setNames(colors.exposure, levels.exposure), guide=FALSE) +
  scale_fill_manual("Exposure Condition", values = setNames(colors.exposure, levels.exposure), guide=FALSE) +
  theme(legend.position = "none", panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())

p.test.proportion = plot_base + 
  geom_dotplot(binaxis = "y", binwidth = 0.015, stackdir = "center", alpha = .3, position = position_dodge(.9)) +
  scale_y_continuous("Proportion of keywords correctly transcribed") +
  coord_cartesian(ylim = c(0.3,1.02))

p.test.proportion
ggsave(p.test.proportion, filename = "../figures/p.exp1a.pdf", device = cairo_pdf, width = 6, height = 4, dpi=300)

p.test.emplogit = plot_base %+%
  (d.exp1a.test %>%
     group_by(WorkerID, Sentence) %>%
     distinct(.keep_all = TRUE) %>%
     group_by(WorkerID, Condition2) %>%
     dplyr::summarise(PropKeywordsCorrect = emplog(mean(PropKeywordsCorrect), 51))) +
  geom_violin(fill = NA) +
  geom_dotplot(binaxis = "y", stackdir = "center", alpha = .3, position = position_dodge(.9)) +
  scale_y_continuous("Empirical logits of correctly transcribed") 

p.test.emplogit
ggsave(p.test.emplogit, filename = "../figures/p.exp1a.emplogit.pdf", device = cairo_pdf, width = 6, height = 4, dpi=300)

```

## Results for Questions 1-3
```{r plots for Exp 1a, fig.width=8, fig.height=4}
plot_3effects(m.exp1a.test.diff, "Experiment 1a", fill = colors.model[1]) 
ggsave("../figures/exp1a_Q123_effects.pdf", width = 8, height = 3.5)

table_3effects(m.exp1a.test.diff)
```

## Sliding difference-coded analyses
```{r, exp1a test block analysis - contrasts for sliding difference coding}
contrasts(d.exp1a.test$Cond.diff)
```

### Model summary
```{r, exp1a test block analysis - model output for sliding difference coding, warning=TRUE, size = "footnotesize"}
summary(m.exp1a.test.diff)
```

### Additional hypothesis tests
```{r, exp1a test block analysis - hypotheses for sliding difference coding}
xhypotheses(list(hypothesis(m.exp1a.test.diff, "Cond.diffTS.vs.MT > 0", class = "b")))
```


## Treatmeant-coded analysis
```{r, exp1a test block analysis - contrasts for treatment coding}
contrasts(d.exp1a.test$Cond.treat)
```


### Model summary
```{r, exp1a test block analysis - model output for treatment coding, warning=TRUE, size = "footnotesize"}
summary(m.exp1a.test.treat)
```

### Additional hypothesis tests

```{r, exp1a test block analysis - hypotheses for treatment coding}
xhypotheses(
  list(
    hypothesis(m.exp1a.test.treat, "Cond.treatTS.vs.CNTL > 0", class = "b"),
    hypothesis(m.exp1a.test.treat, "Cond.treatMT.vs.CNTL > 0", class = "b"),
    hypothesis(m.exp1a.test.treat, "Cond.treatST.vs.CNTL > 0", class = "b")
  )
)
```






\newpage
# Test - Experiment 1b

```{r, exp1b test block analysis - main level - also plot this}
# offset model
m.exp1b.test.diff <- brm(IsCorrect ~ 
                           offset(individual_training_performance.offset) +
                           1 + Cond.diff + 
                           (1 + Cond.diff | Sentence) + (1 | WorkerID),  
                         data = d.exp1b.test, family = bernoulli, cores = max.cores, chains = max.cores, 
                         warmup = nsamples.warmup, iter = nsamples.total, 
                         prior = my.priors, file = "../models/exp1b.Test.diff")
m.exp1b.test.treat <- brm(IsCorrect ~ 
                            offset(individual_training_performance.offset) +
                            1 + Cond.treat +
                            (1 + Cond.treat | Sentence) + (1 | WorkerID),  
                          data = d.exp1b.test, family = bernoulli, cores = max.cores, chains = max.cores, 
                          warmup = nsamples.warmup, iter = nsamples.total, 
                          prior = my.priors, file = "../models/exp1b.Test.treat"
)

myGplot.defaults(type = "paper")
p.test.proportion %+% 
  (d.exp1b.test %>%
     group_by(WorkerID, Sentence) %>%
     distinct(.keep_all = TRUE) %>%
     group_by(WorkerID, Condition2) %>%
     dplyr::summarise(PropKeywordsCorrect = mean(PropKeywordsCorrect))) 
ggsave(filename = "../figures/p.exp1b.pdf", device = cairo_pdf, width = 6, height = 4, dpi=300)

p.test.emplogit %+% (
  d.exp1b.test %>%
    group_by(WorkerID, Sentence) %>%
    distinct(.keep_all = TRUE) %>%
    group_by(WorkerID, Condition2) %>%
    dplyr::summarise(PropKeywordsCorrect = emplog(mean(PropKeywordsCorrect), 51)))
ggsave(filename = "../figures/p.exp1b.emplogit.pdf", device = cairo_pdf, width = 6, height = 4, dpi=300)
```


## Results for Questions 1-3

```{r plots for Exp 1b, fig.width=8, fig.height=4}
plot_3effects(m.exp1b.test.diff, "Experiment 1b", fill = colors.model[1]) 
ggsave("../figures/exp1b_Q123_effects.pdf", width = 8, height = 3.5)

table_3effects(m.exp1b.test.diff)
```

## Sliding difference-coded analyses
```{r, exp1b test block analysis - contrasts for sliding difference coding}
contrasts(d.exp1b.test$Cond.diff)
```

### Model summary
```{r, exp1b test block analysis - model output for sliding difference coding, warning=TRUE, size = "footnotesize"}
summary(m.exp1b.test.diff)
```

### Additional hypothesis tests
```{r, exp1b test block analysis - hypotheses for sliding difference coding}
xhypotheses(list(hypothesis(m.exp1b.test.diff, "Cond.diffMT.vs.ST > 0", class = "b")))
```


## Treatmeant-coded analysis
```{r, exp1b test block analysis - contrasts for treatment coding}
contrasts(d.exp1b.test$Cond.treat)
```


### Model summary
```{r, exp1b test block analysis - model output for treatment coding, warning=TRUE, size = "footnotesize"}
summary(m.exp1b.test.treat)
```

### Additional hypothesis tests

```{r, exp1b test block analysis - hypotheses for treatment coding}
xhypotheses(
  list(
    hypothesis(m.exp1b.test.treat, "Cond.treatTS.vs.CNTL > 0", class = "b"),
    hypothesis(m.exp1b.test.treat, "Cond.treatMT.vs.CNTL > 0", class = "b"),
    hypothesis(m.exp1b.test.treat, "Cond.treatST.vs.CNTL > 0", class = "b")
  )
)
```








\newpage

# Consequences of by-talker variability

Unlike the original study by Bradlow and Bent (2008), our replication included multiple (4) test talkers and many exposure-test talker combinations (20 for the single talker condition, 4 each for the control, multi-talker and talker-specific conditions). This allows us to begin to assess the consequences of cross-talker variability on the effects of exposure. We present additional analyses that account for cross-talker differences, allowing us to assess whether our conclusions would differ if Experiments 1a and 1b had only used a subset of the exposure and test talkers. In particular, we ask two questions:

 + Can *limiting the experiment to one test talker* change the results, compared to having four test talkers?
 + Can *failing to balance the exposure talkers* across the single and multi-talker conditions (as in Bradlow and Bent, 2008) change the results, compared to fully balanced lists (as in the present experiments)? 

We address these questions through additional Bayesian mixed-effects logistic regression, repeating the analyses presented in the main text while including random effects by test talker or exposure-test talker combination. These analyses reveal substantial quantitative---though, at least for our relatively homogenous set of talkers, not *necessarily* qualitative---variability in ou results, depending on the exposure and test talkers. As would be expected, this variability is particularly evident for the single talker and talker-specific conditions, the conditions for which exposure involved only a single talker. In particular, the effect of single talker exposure can vary *qualitatively* depending on the combination of exposure and test talker. <!-- CHECK --> 

This highlights the need for future work to either employ stimuli from multiple talkers, or to appropriately caveat the conclusions drawn from experiments that are based on a very small number of talkers. Put differently, it is to be expected that individual experiments---in particular if they are based on only one exposure and/or test talker---will return different results *even if the experiments employ an adequate number of subjects and items*. Such differences in results do not necessarily reflect empirical conflicts, but might quite simply capture that cross-talker generalization is, or is not, observed after exposure to the *particular* combination of exposure and test talkers. This point deserves emhasis, in particular, given the field's reliance on, and frequent misinterpretation of, the notion of significance: this 'significance filter' (Vasishth, 2018) can misleadingly further the impression that results conflict even when effects across pattern in identical ways. To make things worse, apparent qualitative differences in results between experiments---sometimes within the same paper---are not infrequently used to motivate ad-hoc theorizing. The present results suggest that research on speech perception ought to first ask whether differences in results are simply the consequence of using different combinations of talkers.

Next, we assess the degree of *overconfidence* that results from experiments with just one test talker if we fail to consider that the use of a single test talker makes it impossible to account for cross-talker variability. This further illustrates the consequences of the common practice to measure talker-specific adaptation and cross-talker generalization against a single test talker. Specifically, we ask:

 + How much (if at all) do experiments with a single test talker result in *over-confidence in the generalizability of our results* (if we fail to remember that they are based on just one test talker)?  
 
For this, we analyze subsets of our data that only include one test talker with the same approach as in the main text, and compare the uncertainty about the effect of exposure to results based on the same total amount of data from all four test talkers. The results highlight that the use of a single test talker might make it *appear* as if one can be more certain about the results, but that this relative certainty about the effect of exposure is misleading: with one test talker, researchers simply do not know how much the results would vary if multiple test talkers were considered, and thus researchers might feel overconfident that their striking results will generalize to other talkers.

This leads us to the final analysis in thise section. The use of multiple exposure and test talkers also lets us assess the generalizability of our results beyond the specific exposure and test talkers employed in Experiments 1a and 1b. We use the Bayesian mixed-effects logistic regressions with random effects by exposure and test talkers to quantify how much our certainty about the effect of exposure increases if our analyses acknowledge exposure and/or test talkers as sources of random variability. We find, for example, that the standard error for the effect of single talker exposure increases three-fold <!-- CHECK --> if exposure-test talker combinations are acknowledged as a source of random variability. This further illustrates the need for studies on speech perception---and other domains of the psychological sciences that involve social or socially-conditioned inferences---to use stimuli that come from multiple talkers, and analyses that capture this variability adequately.


## Do differences between test talkers affect the conclusions one would draw?

To address our first question:

 + Can *limiting the experiment to one test talker* change the results, compared to having four test talkers?

we fit an additional sliding difference-coded Bayesian mixed-effects logistic regressions to each of Experiment 1a and 1b. The additional analysis added random intercepts and slopes for the exposure conditions by test talkers. We use these models to estimate the effects of exposure by test talker.^[An alternative approach would be to estimate the effect for each test talker by fitting separate models to the data from each test talker. This approach would not use all data and thus have reduced power. Another approach would be to use all data---as we did here---but to model effects of test talkers as fixed effects, interacting with exposure condition. Empirically, this is unlikely to result in different results than the approach taken here. We prefer the approach we pursued here for three reasons. First, it is conceptually appealing, as it views the four test talkers as sampled from a larger population (among which differences in the effect of exposure are assumed to be normally distributed). Second, treating test talkers as random effects essentially imposes a regularizing prior on the by-test talker means, which should reduce the probability of overfitting to the sample (for an accessible discussion of the resulting 'shrinkage' effects, see Kliegl et al., 2010). Third, the approach is parsimonuous in terms of its degrees of freedom: for a random intercept and a three random slopes for the exposure conditions, the resulting 4x4 variance-covariance matrix of the random effects by test talker has 7 DFs (4 variances, and 3 correlations between them). Treating test talker as fixed effect requires 12 DFs = 3 DFs (for the four test talkers) + 3 x 3 DFs (for the interaction with exposure condition). This benefit becomes even more pronounced for variables with more levels, such as the effects of exposure-test talker combinations: for the random effect approach, the number of DFs does not change, whereit it increases linearly with the number of levels for the fixed effect approach.] 

Models were fit with 3000 warm-up and 2000 posterior samples for each of 4 chains, for a total of 8000 posterior samples. The priors were the same as for the main analysis *except that we set a more specific prior for the intercept*: a student $t$ prior with a scale of 2.5, 7 DFs, and a location parameter set to the posterior mean of the main analysis. This was done to aid convergence, since weaker priors resulted in divergent transitions. 


```{r exp1-TestTalker-random-effects}
# Run the same models but with by-talker random effects. This is conservative (probably overly conservative)
# compared to previous work). More regularizing priors were needed to converge --- in particular for the
# intercept. The primary contributor to convergence seems to not be the mean of the intercept prior (the 
# data overrides that quite easily), but having a less broad prior, ruling out more extreme values for the 
# intercept.
nsamples.warmup.wTalker = 3000
nsamples.total.wTalker = 5000

# To aid convergence stronger priors had to be used, compared to the models without additional random effects
my.priors.wTalker.diff = c(
  # Prior on intercept was necessary for convergence. 
  # Set to value observed in models without talker random effects
  prior(student_t(3,2.12,2.5), class = "Intercept"),
  # Following Gelman et al (2008) and later revisions on wiki.
  prior(student_t(3,0,2.5), class = b),
  # I used a smaller scale to aid convergence, 5 is a common choice for logit models
  prior(cauchy(0,5), class = sd), 
  # Uniform prior of correlations
  prior(lkj(1), class = cor)
)
m.exp1a.test.diff.wTestTalker <- brm(
  IsCorrect ~ Cond.diff + offset(individual_training_performance.offset) + 
    (1 + Cond.diff | Sentence) + (1 | WorkerID) + (1 + Cond.diff | TestTalkerID),  
  data = d.exp1a.test, family = bernoulli, cores = max.cores, chains =  4, 
  warmup = nsamples.warmup.wTalker, iter = nsamples.total.wTalker, 
  control = list(adapt_delta = 0.99995, max_treedepth = 15),  
  prior = my.priors.wTalker.diff, 
  file = "../models/exp1a.Test.diff.wTestTalker")


###################################### Experiment 1b
my.priors.wTalker.diff = c(
  # Prior on intercept was necessary for convergence.
  prior(student_t(3,2.20,2.5), class = "Intercept"),
  # Following Gelman et al (2008) and later revisions on wiki.
  prior(student_t(3,0,2.5), class = b),
  # I used a smaller scale to aid convergence, 5 is a common choice for logit models
  prior(cauchy(0,5), class = sd), 
  # Uniform prior of correlations
  prior(lkj(1), class = cor)
)
m.exp1b.test.diff.wTestTalker <- brm(
  IsCorrect ~ Cond.diff + offset(individual_training_performance.offset) + 
    (1 + Cond.diff | Sentence) + (1 | WorkerID) + (1 + Cond.diff | TestTalkerID),  
  data = d.exp1b.test, family = bernoulli, cores = max.cores, chains = 4, 
  warmup = nsamples.warmup.wTalker, iter = nsamples.total.wTalker, 
  control = list(adapt_delta = 0.9999, max_treedepth = 15),  
  prior = my.priors.wTalker.diff, file = "../models/exp1b.Test.diff.wTestTalker")
```


### Summary of findings 
It is clear that the specific effects differ between test talkers. For the present data, these differences are comparatively small: we find the same ordering of relative effect sizes across all test talkers, and the posterior probability for a positive answer to Questions 1-3 is always larger than 50%. The similarity of the effect sizes is perhaps not surprising, since we (a) all of the six test talkers we considered in our pilot study were chosen to be of medium proficiency, <!-- Xin: CHECK --> and (b) of those six, we chose the four test talkers that were not close to ceiling after talker-specific adaptation. In short, our test talkers are representive only of a comparatively homogenous subset of the overall population of Mandarin-accented talkers.

However, even for test talkers like these, who are *a priori* expected to be relatively similar to each other, it is also clear that we would have come to at least gradiently different conclusions if we had chosen only one of our four test talkers. <!-- TO DO: continue here with brief summary of results -->

### Experiment 1a
We first quantify how the overall result of exposure differs depending on the test talker. This can be visualized by plotting the predicted transcription accuracy during test for each of the four exposure conditions depending on the test talker. In these predictions we do *not* include the offset term that corrects for individual differences, so that the plot shows the predicted differences between conditions *if all participants had the same offset (of 0).* The dotplot shows the empirical distribution of all participants. The pointrange shows the mean and 95% highest posterior density interval of the posterior distribution of the predicted transcription accuracy.

```{r, fig.height=3.5, fig.width=15}
plot_predicted_outcome(
  data = d.exp1a.test,
  model = m.exp1a.test.diff.wTestTalker,
  groups = "TestTalkerID",
  n.draws = 2000,
  experiment = "1a")
```

We can also conduct the Bayesian hypothesis tests for Questions 1-3 separately for each test talker. Qualitatively, we find that that, four all four test talkers, the effects of all four comparisons went in the same direction---i.e., exposure always had *positive* effects on comprehension accuracy. We further find the same ordering of effect sizes for the four comparisons of exposure conditions for three out of the four test talkers (for the fourth test talker, the relative effect sizes for questions 2 and 3 order differently). The same holds for ordering of the strength of evidence (Bayes factors).  However, talker-specific adaptation was the only effect that reached *strong* support for all four test talkers. Generalization after multi-talker exposure received strong support for three out of the four test talkers, and positive support on the fourt test talker. Generalization after single talker exposure received strong support for only one out of the four test talkers, and positive support for the other three test talkers. Overall, test talker 032 and 037 showed the strongest effects, and test talker 034 showed the weakest effects.

```{r}
plot_3effects(m.exp1a.test.diff.wTestTalker, "Experiment 1a by test talker", 
              by = "TestTalker", fill = colors.model[2], alpha = 1) 
ggsave(filename = "../figures/exp1a_Q123_effects_byTestTalker.pdf", width = 12, height = 3.5)

table_3effects(m.exp1a.test.diff.wTestTalker, 
               scope = "coef", group = "TestTalkerID")
```

Next, we further quantify the difference between the different test talkers. The following hypothesis tests assess whether the effect inferred for a given test talker differs from the overall effect inferred across test talkers. This was never the case for any of the comparisons or any of the test talkers. It is worth noting though that *all* effects were numerically smaller for test talker 043.

```{r}
table_3effects(m.exp1a.test.diff.wTestTalker, 
               scope = "ranef", group = "TestTalkerID")
```


### Experiment 1b
We first quantify how the overall result of exposure differs depending on the test talker. As in Experiment 1a, exposure always had *positive* effects on comprehension accuracy. The relative effect sizes order the same way as in Experiment 1a for two of the test talkers (032 and 035), but not the other two test talkers. The most striking difference is found for test talker 037, which exhibited the second strongest support for generalization after single talker exposure in Experiment 1a, but only exhibited positive support in Experiment 1b.

```{r, fig.height=3.5, fig.width=15}
plot_predicted_outcome(
  data = d.exp1b.test,
  model = m.exp1b.test.diff.wTestTalker,
  groups = "TestTalkerID",
  n.draws = 2000,
  experiment = "1b")
```

```{r}
plot_3effects(m.exp1b.test.diff.wTestTalker, "Experiment 1b by test talker", 
              by = "TestTalker", fill = colors.model[2], alpha = 1)
ggsave(filename = "../figures/exp1a_Q123_effects_byTestTalker.pdf", width = 12, height = 3.5)

table_3effects(m.exp1b.test.diff.wTestTalker, 
               scope = "coef", group = "TestTalkerID")
```

Next, we quantify the difference between the different test talkers. The following hypothesis tests assess whether the effect inferred for a given test talker differs from the overall effect inferred across test talkers. As was the case for Experiment 1a, there never was anywhere close to strong evidence that effects for a given test talker differed from the overall effect. For test talker 043, for which all effects were numerically smaller than the average in Experiment 1a, three out of four effects were numerically smaller than the average in Experiment 1b.

```{r}
table_3effects(m.exp1b.test.diff.wTestTalker, 
               scope = "ranef", group = "TestTalkerID")
```


## Do differences between exposure-test talker combinations affect the conclusions one would draw? 
```{r exp1-ExposureTestTalker-random-effects}
###################################### Experiment 1a
my.priors.wExposureTalker.diff = c(
  # Prior on intercept was necessary for convergence.
  prior(student_t(10,2.12,2.5), class = "Intercept"),
  # Following Gelman et al (2008) and later revisions on wiki.
  prior(student_t(3,0,2.5), class = b),
  # I used a smaller scale to aid convergence, 5 is a common choice for logit models
  prior(cauchy(0,2.5), class = sd), 
  # Uniform prior of correlations
  prior(lkj(1), class = cor)
)

# m.exp1a.test.diff.wExposure_embedded_under_TestTalker <- brm(IsCorrect ~ Cond.diff + offset(individual_training_performance.offset) +
#                                    (1 + Cond.diff | Sentence) + (1 | WorkerID) +
#                                    (1 + Cond.diff | TestTalkerID / TrainingUnderTestTalkerID),
#                                  data = d.exp1a.test, family = bernoulli, cores = max.cores, chains = 4,
#                                  warmup = nsamples.warmup.wTalker, iter = nsamples.total.wTalker,
#                                  control = list(adapt_delta = 0.9999, max_treedepth = 15),
#                                  prior = my.priors.wTalker.diff, file = "../models/exp1a.Test.diff.wExposure_embedded_under_TestTalker")

m.exp1a.test.diff.wExposure_crossed_with_TestTalker <- brm(
  IsCorrect ~ Cond.diff + offset(individual_training_performance.offset) +
    (1 + Cond.diff | Sentence) + (1 | WorkerID) +
    (1 + Cond.diff | TestTalkerID) + 
    (1 | TrainingUnderTestTalkerID),
  data = d.exp1a.test, 
  family = bernoulli, cores = max.cores, chains = 4,
  warmup = nsamples.warmup.wTalker, iter = nsamples.total.wTalker,
  control = list(adapt_delta = 0.99999, max_treedepth = 15),
  prior = my.priors.wTalker.diff, file = "../models/exp1a.Test.diff.wExposure_crossed_with_TestTalker")

###################################### Experiment 1b
my.priors.wExposureTalker.diff = c(
  # Prior on intercept was necessary for convergence.
  prior(student_t(10,2.20,2.5), class = "Intercept"),
  # Following Gelman et al (2008) and later revisions on wiki.
  prior(student_t(3,0,2.5), class = b),
  # I used a smaller scale to aid convergence, 5 is a common choice for logit models
  prior(cauchy(0,2.5), class = sd), 
  # Uniform prior of correlations
  prior(lkj(1), class = cor)
)

# m.exp1b.test.diff.wExposureTestTalker <- brm(IsCorrect ~ Cond.diff + offset(individual_training_performance.offset) + 
#                                    (1 + Cond.diff | Sentence) + (1 | WorkerID) + 
#                                    (1 + Cond.diff | TestTalkerID / TrainingTalkerID),  
#                                  data = d.exp1b.test, family = bernoulli, cores = max.cores, chains = 4, 
#                                  warmup = nsamples.warmup.wTalker, iter = nsamples.total.wTalker, 
#                                  control = list(adapt_delta = 0.9999, max_treedepth = 15),  
#                                  prior = my.priors.wExposureTestTalker.diff, file = "../models/exp1b.Test.diff.wExposureTestTalker")

m.exp1b.test.diff.wExposure_crossed_with_TestTalker <- brm(
  IsCorrect ~ 
    Cond.diff + offset(individual_training_performance.offset) +
    (1 + Cond.diff | Sentence) + (1 | WorkerID) +
    (1 + Cond.diff | TestTalkerID) + 
    (1 | TrainingUnderTestTalkerID),
  data = d.exp1b.test, 
  family = bernoulli, cores = max.cores, chains = 4,
  warmup = nsamples.warmup.wTalker, iter = nsamples.total.wTalker,
  control = list(adapt_delta = 0.99999, max_treedepth = 15),
  prior = my.priors.wExposureTalker.diff, 
  file = "../models/exp1b.Test.diff.wExposure_crossed_with_TestTalker")
```

To address our second question:

 + Can *failing to balance the exposure talkers* across the single and multi-talker conditions (as in Bradlow and Bent, 2008) change the results, compared to fully balanced lists (as in the present experiments)? 

we fit an additional sliding difference-coded Bayesian mixed-effects logistic regressions to each of Experiment 1a and 1b. This analysis further added random intercepts by the combination of exposure and test talker (coded as "0" for all but the single talker condition). This was done in addition to the random intercepts and slopes by test talker added in the previous section. This added one additional parameter to the analysis (the standard deviation of the random intercept by combination of exposure and test talker) to model the 21 unique random intercepts (20 unique combinations of exposure and test talker in the single talker condition + "0"). By adding a random intercept by combination of exposure and test talker, rather than by exposure talker, we can capture that the same exposure talker might have a different effect on transcription accuracy during test depending on the test talker. This avoids the assumption that the effects of exposure and test talkers are independent of each other.^[Note that it was impossible to include random slopes for the exposure condition by combination of exposure and test talker since exposure condition varied *between* combinations of exposure and test talker.]

### Experiment 1a
We first plot the predicted transcription accuracy during test for each of the four exposure conditions, depending on the exposure and test talker. The five different exposure talkers for the single talker condition of each test talker are shown as separate pointranges. Within each test talker, the exposure talkers are alphanumerically sorted in ascending order from left to right. For example, for test talker 035, the five point ranges correspond to (from left to right): 016, 021, 032, 037, and 043.

This plot shows that there is substantial variability across the combinations of exposure and test talkers. Even within each test talker, there is substantial variability across the different exposure talkers in the single talker condition. At the same time, the analysis suggests high uncertainty about the predicted transcription accuracy in the single talker condition. This is not surprising given that each experiment only had four participants for each unique combination of exposure and test talker in the single talker condition. We thus do not necessarily expect the same pattern of differences in the Experiment 1b.

```{r, fig.height=3.5, fig.width=15}
plot_predicted_outcome(
  data = d.exp1a.test,
  model = m.exp1a.test.diff.wExposure_crossed_with_TestTalker,
  groups = c("TestTalkerID", "TrainingUnderTestTalkerID"),
  n.draws = 2000,
  experiment = "1a")
```

Next, we plot the Bayesian hypotheses tests for Questions 1-3. Note that the results for talker-specific adaptation and Question 1 (comparing the multi-talker condition against the control) do not change much. The results for Questions 2 and 3, however, exhibit somewhat larger changes in the updated model---presumably because both of these questions depend on the predicted mean of the single talker condition. Overall, the results are pretty much the same as in the model in the previous section, except that we now do not any longer have strong support for a positive answer to Question 2 in any of the four different test talkers (whereas there was strong support for a positive answer for test talker 035 in the model withtout random effects by exposure-test talker combination.

```{r, fig.height=20, fig.width=20}
# plot_3effects(m.exp1a.test.diff.wExposure_crossed_with_TestTalker,
#               "Experiment 1a by exposure-test talker combination",
#               by = "ExposureTestTalker", fill = colors.model[3])
# ggsave(filename = "../figures/exp1a_Q123_effects_byExposureTestTalker.pdf", width = 20, height = 20)

table_3effects(m.exp1a.test.diff.wExposure_crossed_with_TestTalker,
               scope = "coef", group = "TestTalkerID")
table_3effects(m.exp1a.test.diff.wExposure_crossed_with_TestTalker,
               scope = "ranef", group = "TestTalkerID")
```

### Experiment 1b

Plotting the predicted effects, we again see variability across the different combinations of exposure and test talker as well as substantial uncertainty about these differences. 

```{r, fig.height=3.5, fig.width=15}
plot_predicted_outcome(
  data = d.exp1b.test,
  model = m.exp1b.test.diff.wExposure_crossed_with_TestTalker,
  groups = c("TestTalkerID", "TrainingUnderTestTalkerID"),
  n.draws = 2000,
  experiment = "1b")
```

Replicating the analysis by test talkers, we find that only the talker-specific effect receives strong support within each test talker.

```{r, fig.height=20, fig.width=20}
table_3effects(m.exp1b.test.diff.wExposure_crossed_with_TestTalker,
               scope = "coef", group = "TestTalkerID")
table_3effects(m.exp1b.test.diff.wExposure_crossed_with_TestTalker,
               scope = "ranef", group = "TestTalkerID")
```




## Uncertainty about posterior effect size estimates based on the random effect structure

We ask how the uncertainty about the effects of exposure changes depending on which grouping factors are acknowledged in the analysis. The analyses reported in the main text followed the conventions of the field in 2020 and included by-subject and -item random effects (Bradlow and Bent, 2008 only included by-subject analyses). However, our experiment included multiple test talkers as well as many exposure-test talker combinations. If we want to know whether the effects of our exposure conditions are likely to generalize across the population of talkers that our exposure and test talkers are drawn from, we arguably need to adequately account for these grouping factors (see Yarkoni, 2019 for discussion).

We thus use the two Bayesian mixed-effects logistic regressions with random effects by test talker and exposure-test talker combinations to measure how the uncertainty about the effect of exposure condition changes if those additional random effects are included in the analysis. 

### Accounting for variability by test talker

We first summarize the results for the Bayesian hypothesis tests for Questions 1-3 for Experiments 1a and 1b when random effects for test talker are included.

```{r}
table_3effects(m.exp1a.test.diff.wTestTalker, scope = "standard")
table_3effects(m.exp1b.test.diff.wTestTalker, scope = "standard")
```

### Accounting for variability by exposure-test talker combination

Next we summarize the results for the Bayesian hypothesis tests for Questions 1-3 for Experiments 1a and 1b when random effects for exposure-test talker combinations are included.

```{r}
table_3effects(m.exp1a.test.diff.wExposure_crossed_with_TestTalker, scope = "standard")
table_3effects(m.exp1b.test.diff.wExposure_crossed_with_TestTalker, scope = "standard")
```

### Visualizing the increase in uncertainty 

The following figures plot the posterior distribution of exposure effects for the three models, and quantify the resulting uncertainty as the standard deviation of that distribution (i.e., the standard error of the estimated effect).

With each additional source of variability (and thus uncertainty), we see that the uncertainty about the effect of exposure increases. This isn't surprising (for relevant background, see Yarkoni, 2019), but it is still worth noting that these increases are *substantial*: the standard error of the effect of single talker, compared to control, exposure increases almost three- to four-fold when we account for the variability across test talkers. <!-- CHECK whether result still holds -->

```{r uncertainty-by-RE-structure, fig.width=12, fig.height=4}
# Experiment 1a
models.diff = list("no by-talker" = m.exp1a.test.diff, 
                   "by test talker" = m.exp1a.test.diff.wTestTalker, 
                   "by exposure &\ntest talker" = m.exp1a.test.diff.wExposure_crossed_with_TestTalker)

p = plot_3effects(models.diff[[3]], title = "Experiment 1a", alpha = .5, fill = colors.model[3], .width = NULL) 
p = plot_3effects(models.diff[[2]], add_plot = p, alpha = .5, fill = colors.model[2], .width = NULL) 
p = plot_3effects(models.diff[[1]], add_plot = p, alpha = .5, fill = colors.model[1], .width = NULL) 
p = plot_3effects_var(models.diff, add_plot = p, rel_widths = c(.75, .25))

p
ggsave(p, filename = "../figures/exp1a_Q123_effects_dependingOnRE.pdf", width = 12, height = 4.5)

# Experiment 1b
models.diff = list("no by-talker" = m.exp1b.test.diff, 
                   "by test talker" = m.exp1b.test.diff.wTestTalker, 
                   "by exposure &\ntest talker" = m.exp1b.test.diff.wExposure_crossed_with_TestTalker)

p = plot_3effects(models.diff[[3]], title = "Experiment 1b", alpha = 1, fill = colors.model[3], .width = NULL) 
p = plot_3effects(models.diff[[2]], add_plot = p, alpha = .5, fill = colors.model[2], .width = NULL) 
p = plot_3effects(models.diff[[1]], add_plot = p, alpha = .25, fill = colors.model[1], .width = NULL) 
p = plot_3effects_var(models.diff, add_plot = p, rel_widths = c(.75, .25))

p
ggsave(p, filename = "../figures/exp1b_Q123_effects_dependingOnRE.pdf", width = 12, height = 4.5)
```



## Full model summaries
Finally, we provide the full model output for the models with random effects by test talker and exposure-test talker combination.

### Random effect by test talker

#### Experiment 1a

```{r, exp1a test block analysis - model output for sliding difference coding w/ random effects by test talker, warning=T, size = "footnotesize"}
summary(m.exp1a.test.diff.wTestTalker)
```

#### Experiment 1b

```{r, exp1b test block analysis - model output for sliding difference coding w/ random effects by test talker, warning=T, size = "footnotesize"}
summary(m.exp1b.test.diff.wTestTalker)
```

### Random effect by exposure and by test talker

#### Experiment 1a

```{r, exp1a test block analysis - model output for sliding difference coding w/ random effects by exposure and test talker, warning=T, size = "footnotesize"}
summary(m.exp1a.test.diff.wExposure_crossed_with_TestTalker)
```

#### Experiment 1b

```{r, exp1b test block analysis - model output for sliding difference coding w/ random effects by exposure and test talker, warning=T, size = "footnotesize"}
summary(m.exp1b.test.diff.wExposure_crossed_with_TestTalker)
```



\newpage
# Performance across time (2-3 year span)

Experiments 1a and 1b were run at different times, almost 2 years apart. In both case, we recruited an initial number of participants and then completed to fill up lists until the pre-specified number of participants per condition was reached. This left us with data from participants that had taken the experiment at different times.

## Exposure block

```{r, training across time}
# DaysSinceFirst is set so that 1 means first day. (so 'days since start of experiment')
myGplot.defaults(type = "paper")

# Training performance
d.exp1ab.training %>% 
  filter(Quintile == 0) %>%
  mutate(DateDay = as_datetime(assignmentsubmittime), DaysSinceFirst = (DateDay - min(DateDay) + 1) %/% ddays(1)) %>% 
  group_by(WorkerID, Condition2) %>% 
  dplyr::summarise(Correct = mean(IsCorrect), days = first(DaysSinceFirst)) %>% 
  ggplot(aes(x = days + 1, y= Correct, color = Condition2)) + 
  stat_summary(fun.data = mean_cl_boot, geom= "pointrange", alpha = .8, size = .5) + 
  geom_smooth(method = "lm") + 
  scale_x_log10("Days since first data collection") +
  scale_y_continuous("Proportion of keywords correctly transcribed") +
  scale_colour_manual("Exposure Condition", 
                      values = setNames(colors.exposure, levels.exposure)) +
  theme(legend.position = "bottom")
ggsave(filename = "../figures/Change in exposure performance (1st Quintile) over submit times.pdf", device = cairo_pdf, width = 6.5, height = 4.2, dpi=300)

d.exp1ab.training %>% 
  filter(Quintile == 0) %>%
  mutate(DateDay = as_datetime(assignmentsubmittime), DaysSinceFirst = (DateDay - min(DateDay) + 1) %/% ddays(1)) %>% 
  group_by(WorkerID, Condition2) %>% 
  dplyr::summarise(Correct = emplog(mean(IsCorrect), 51), days = first(DaysSinceFirst)) %>% 
  ggplot(aes(x = days + 1, y= Correct, color = Condition2)) + 
  stat_summary(fun.data = mean_cl_boot, geom= "pointrange", alpha = .8, size = .5) + 
  geom_smooth(method = "lm") + 
  scale_x_log10("Days since first data collection") +
  scale_y_continuous("Empirical logits of correctly transcribed") +
  scale_colour_manual("Exposure Condition", 
                      values = setNames(colors.exposure, levels.exposure)) +
  theme(legend.position = "bottom")
ggsave(filename = "../figures/Change in exposure performance (1st Quintile) over submit times-emplogit.pdf", device = cairo_pdf, width = 6.5, height = 4.2, dpi=300)
```


## Test block

```{r, test across time }
d.exp1ab.test %>% 
  mutate(DateDay = as_datetime(assignmentsubmittime), DaysSinceFirst = (DateDay - min(DateDay) + 1) %/% ddays(1)) %>% 
  group_by(WorkerID, Condition2) %>% 
  dplyr::summarise(Correct = mean(IsCorrect), days = first(DaysSinceFirst)) %>% 
  ggplot(aes(x = days + 1, y= Correct, color = Condition2)) + 
  stat_summary(fun.data = mean_cl_boot, geom= "pointrange", alpha = .8, size = .5) + 
  geom_smooth(method = "lm") + 
  scale_x_log10("Days since first data collection") +
  scale_y_continuous("Proportion of keywords correctly transcribed") +
  scale_colour_manual("Exposure Condition", 
                      values = setNames(colors.exposure, levels.exposure)) +
  theme(legend.position = "bottom")
  
ggsave(filename = "../figures/Change in test performance over submit times.pdf", device = cairo_pdf, width = 6.5, height = 4.2, dpi=300)


d.exp1ab.test %>% 
  mutate(DateDay = as_datetime(assignmentsubmittime), DaysSinceFirst = (DateDay - min(DateDay) + 1) %/% ddays(1)) %>% 
  group_by(WorkerID, Condition2) %>% 
  dplyr::summarise(Correct = emplog(mean(IsCorrect), 51), days = first(DaysSinceFirst)) %>% 
  ggplot(aes(x = days + 1, y= Correct, color = Condition2)) + 
  stat_summary(fun.data = mean_cl_boot, geom= "pointrange", alpha = .8, size = .5) + 
  geom_smooth(method = "lm") + 
  scale_x_log10("Days since first data collection") +
  scale_y_continuous("Empirical logits of correctly transcribed") +
  scale_colour_manual("Exposure Condition", 
                      values = setNames(colors.exposure, levels.exposure)) +
  theme(legend.position = "bottom")
  
ggsave(filename = "../figures/Change in test performance (1st Quintile) over submit times-emplogit.pdf", device = cairo_pdf, width = 6.5, height = 4.2, dpi=300)


```






\newpage

# Meta-analysis - Combined data of Experiments 1a and 1b
We combine the data from both experiments and then repeats the same analysis as above. This is the same as updating the posterior of Experiment 1a with the data from Experiment 1b, therefore assuming exchangeability.

```{r, exp1ab.noAdditionalPredictors test block analysis - main level - also plot this}
d.exp1ab.test$Experiment = factor(d.exp1ab.test$Experiment,
                                  levels = c("1a", "1b"))
contrasts(d.exp1ab.test$Experiment) = cbind("1a vs. 1b" = c(.5,-.5))

m.exp1ab.noAdditionalPredictors.test.diff <- brm(
  IsCorrect ~ 
    offset(individual_training_performance.offset) + Cond.diff + 
    (1 + Cond.diff | Sentence) + (1 | WorkerID),  
  data = d.exp1ab.test, family = bernoulli, cores = max.cores, chains = max.cores, 
  warmup = nsamples.warmup, iter = nsamples.total, 
  prior = my.priors, file = "../models/exp1ab.noAdditionalPredictors.Test.diff")

# Without any correction
# m.exp1ab.noAdditionalPredictors.test.diff.woCorrection <- brm(
#   IsCorrect ~ 
#     Cond.diff + 
#     (1 + Cond.diff | Sentence) + (1 | WorkerID),  
#   data = d.exp1ab.test, family = bernoulli, cores = max.cores, chains = max.cores, 
#   warmup = nsamples.warmup, iter = nsamples.total, 
#   prior = my.priors, file = "../models/exp1ab.noAdditionalPredictors.Test.diff.woCorrection")

myGplot.defaults(type = "paper")
p.test.proportion %+%
  (d.exp1ab.test %>%
  group_by(Experiment, WorkerID, Sentence) %>%
  distinct(.keep_all = TRUE) %>%
  group_by(Experiment, WorkerID, Condition2) %>%
	dplyr::summarise(PropKeywordsCorrect = mean(PropKeywordsCorrect))) + 
  aes(shape = Experiment) + theme(legend.position = "bottom")
ggsave(filename = "../figures/p.exp1ab.noAdditionalPredictors_split.pdf", device = cairo_pdf, width = 8, height = 4.3, dpi=300)

p.test.emplogit %+% 
  (d.exp1ab.test %>%
  group_by(Experiment, WorkerID, Sentence) %>%
  distinct(.keep_all = TRUE) %>%
  group_by(Experiment, WorkerID, Condition2) %>%
	dplyr::summarise(PropKeywordsCorrect = emplog(mean(PropKeywordsCorrect), 51))) + 
  aes(shape = Experiment) + theme(legend.position = "bottom")
ggsave(filename = "../figures/p.exp1ab.noAdditionalPredictors_split.emplogit.pdf", device = cairo_pdf, width = 8, height = 4.3, dpi=300)
```


## Results for Questions 1-3
```{r plots for Exp 1ab.noAdditionalPredictors, fig.width=8, fig.height=4}
plot_3effects(m.exp1ab.noAdditionalPredictors.test.diff, "Experiments 1a and 1b", fill = colors.model[1]) 
ggsave("../figures/exp1ab_Q123_effects.noAdditionalPredictors.pdf", width = 8, height = 3.5)

table_3effects(m.exp1ab.noAdditionalPredictors.test.diff)
```

## Model summary

```{r, exp1ab.noAdditionalPredictors test block analysis - model output for sliding difference coding, warning=TRUE}
summary(m.exp1ab.noAdditionalPredictors.test.diff)
```

## Additional hypothesis testing
```{r, exp1ab.noAdditionalPredictors test block analysis - hypotheses for sliding difference coding}
xhypotheses(list(hypothesis(m.exp1ab.noAdditionalPredictors.test.diff, "Cond.diffTS.vs.MT > 0", class = "b")))
```












# Replication analysis

Use the previous data to establish a prior. You may want to construct a normal prior centered at their estimate, with a standard deviation equal to their SD (Martin proposed SE) estimate. Do this for every coefficient. Construct two models, WITHOUT using the ~ syntax. You need to use the target += normal_lpdf(…) syntax. (This is because we will need to compute the marginal likelihood, with all normalizing constants present, and the ~ syntax drops those constants). The first model has the full model, with priors defined in step 1. The second model has the reduced model, with the coefficient of interest implicitly set to zero (as in, it isn’t present). Everything else is the same as in the first model. Estimate both models using the replication data.

```{r, set up replication analysis based on bridge sampling}
# High number of posterior samples required for bridge_sampling
nsamples.warmup = 1000
nsamples.total = nsamples.total.max

## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Step 1: set priors and contrasts
## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Let's scale continuous variables to have SD .5 (i.e. divide through 2 SDs) and set mean to maximally 
# ambiguous continuum step (rather than center). Binary factors should be coded to have level-
# distance of 1. Then we follow https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations 
# and set the scale of the our t-prior to 2.5
my.priors = c(
    # Following Gelman et al (2008) and later revisions on wiki.
    # First parameter is DFs: smaller means fatter (less normal) tails. As DFs approach, distribution approaches normality
    # Second parameter is mu: should be 0 to be unbiased
    # Third parameter is sigma: larger means wider
    prior(student_t(3,0,2.5), class = b),
    # 5 is a common choice for logit models. Consider using a stronger prior if convergence is an issue
    prior(cauchy(0,5), class = sd), 
    # Uniform prior of correlations
    prior(lkj(1), class = cor)
)

d.exp1a.test = d.exp1ab.test %>% filter(Experiment == "1a") %>% defineContrasts()
d.exp1b.test = d.exp1ab.test %>% filter(Experiment == "1b") %>% defineContrasts()
```


## Questions 1-2 + Talker-specific adaptation

```{r, replication analysis with treatment coding, message=F, warning=F}
## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Step 2: Run model on original data. But with numerically coded variables instead of contrast
## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
m.exp1a.test.treat <- brm(
  IsCorrect ~ offset(individual_training_performance.offset) + 
    Cond.treatTS.vs.CNTL + Cond.treatMT.vs.CNTL + Cond.treatST.vs.CNTL + 
    (1 + Cond.treatTS.vs.CNTL + Cond.treatMT.vs.CNTL + Cond.treatST.vs.CNTL | Sentence) + 
    (1 | WorkerID),  
  data = d.exp1a.test, family = bernoulli, 
  iter = nsamples.total, warmup = nsamples.warmup, 
  chains = max.cores, cores = max.cores,
  prior = my.priors, 
  save_model = "../models/m.exp1a.test.treat.full.numerically_coded_conditions.stan", 
  file = "../models/exp1a.Test.treat.numerical_coded_conditions"
)

# Let's look at this model
# stancode(m.exp1a.test.treat)

## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Step 3: Derive priors from original data.
## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
model = m.exp1a.test.treat
p = posterior_samples(model)


p %<>%
  dplyr::select(-starts_with("r_")) %>%
  dplyr::select(-starts_with("lp_")) %>%
  summarise_all(
    .funs = c("mean", "mode", "sd", "se") 
  ) %>%
  t() %>%
  as.data.frame() %>%
  rownames_to_column(var = "coef") %>%
  mutate(
    property = gsub("^.*_([a-z]+)$", "\\1", coef),
    coef = gsub("^(.*)_[a-z]+$", "\\1", coef)
  ) %>%
  spread(property, V1) %>%
  mutate(
    class = gsub("^([a-z]+)_.*$", "\\1", coef),
    coef = gsub("^[a-z]+_(.*)$", "\\1", coef),
    group = ifelse(gsub("^([A-Za-z]+)__.*$", "\\1", coef) %in% c("Sentence", "WorkerID"), gsub("^([A-Za-z]+)__.*$", "\\1", coef), ""),
    coef = gsub("^[A-Za-z]+__(.*)$", "\\1", coef),
    prior = case_when(
      class == "b" ~ paste0("normal(", mean, ",", sd, ")"),
      class == "sd" ~ paste0("normal(", mean, ",", sd, ")")
    )
  ) %>% 
  filter(class != "cor") %>%                          # For now removing correlation priors
  dplyr::select(prior, class, coef, group) %>%
  mutate(
    class = ifelse(coef == "Intercept" & class == "b", "Intercept", class),
    coef = ifelse(class == "Intercept", "", coef)
  )

my.newpriors = c(
  set_prior(p[1,"prior"], class = p[1,"class"], coef = p[1,"coef"], group = p[1,"group"]),
  set_prior(p[2,"prior"], class = p[2,"class"], coef = p[2,"coef"], group = p[2,"group"]),
  set_prior(p[3,"prior"], class = p[3,"class"], coef = p[3,"coef"], group = p[3,"group"]),
  set_prior(p[4,"prior"], class = p[4,"class"], coef = p[4,"coef"], group = p[4,"group"]),
  set_prior(p[5,"prior"], class = p[5,"class"], coef = p[5,"coef"], group = p[5,"group"]),
  set_prior(p[6,"prior"], class = p[6,"class"], coef = p[6,"coef"], group = p[6,"group"]),
  set_prior(p[7,"prior"], class = p[7,"class"], coef = p[7,"coef"], group = p[7,"group"]),
  set_prior(p[8,"prior"], class = p[8,"class"], coef = p[8,"coef"], group = p[8,"group"]),
  set_prior(p[9,"prior"], class = p[9,"class"], coef = p[9,"coef"], group = p[9,"group"]),
#  set_prior(p[10,"prior"], class = p[10,"class"], coef = p[10,"coef"], group = p[10,"group"]),
  prior(lkj(1), class = cor)
)
my.newpriors.woTS.vs.CNTL = my.newpriors[-3,]
my.newpriors.woMT.vs.CNTL = my.newpriors[-1,]
my.newpriors.woST.vs.CNTL = my.newpriors[-2,]


## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Step 4: Run same numerically-coded model on new data, but with priors based on original data  
## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
m.exp1b.test.treat.full <- brm(
  IsCorrect ~ offset(individual_training_performance.offset) + 
    Cond.treatTS.vs.CNTL + Cond.treatMT.vs.CNTL + Cond.treatST.vs.CNTL + 
    (1 + Cond.treatTS.vs.CNTL + Cond.treatMT.vs.CNTL + Cond.treatST.vs.CNTL | Sentence) +
    (1 | WorkerID),  
  data = d.exp1b.test, family = bernoulli, 
  iter = nsamples.total, warmup = nsamples.warmup, 
  chains = max.cores, cores = max.cores,
  prior = my.newpriors, save_all_pars = T, 
  save_model = "../models/m.exp1b.test.treat.full.numerically_coded_conditions.stan", 
  file = "../models/exp1b.Test.treat.numerical_coded_conditions"
)


## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Step 4: Run all three 'nullmodels' -- i.e. the same numerically-coded model on new data, but 
#         with one parameter set to 0 (removed). Again use priors based on original data. 
#         
#         NB: We're keeping the random effects for the null effect in the model.
## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
m.exp1b.test.treat.woTS.vs.CNTL <- brm(
  IsCorrect ~ offset(individual_training_performance.offset) + 
    Cond.treatMT.vs.CNTL + Cond.treatST.vs.CNTL + 
    (1 + Cond.treatTS.vs.CNTL + Cond.treatMT.vs.CNTL + Cond.treatST.vs.CNTL | Sentence) + 
    (1 | WorkerID),  
  data = d.exp1b.test, family = bernoulli, 
  iter = nsamples.total, warmup = nsamples.warmup, 
  chains = max.cores, cores = max.cores,
  prior = my.newpriors.woTS.vs.CNTL, save_all_pars = T, 
  save_model = "../models/m.exp1b.test.treat.woTS.vs.CNTL.numerically_coded_conditions.stan",
  file = "../models/exp1b.Test.treat.woTS.vs.CNTL.numerical_coded_conditions"
)
m.exp1b.test.treat.woMT.vs.CNTL <- brm(
  IsCorrect ~ offset(individual_training_performance.offset) + 
    Cond.treatTS.vs.CNTL + Cond.treatST.vs.CNTL + 
    (1 + Cond.treatTS.vs.CNTL + Cond.treatMT.vs.CNTL + Cond.treatST.vs.CNTL | Sentence) + 
    (1 | WorkerID),  
  data = d.exp1b.test, family = bernoulli, 
  iter = nsamples.total, warmup = nsamples.warmup, 
  chains = max.cores, cores = max.cores,
  prior = my.newpriors.woMT.vs.CNTL, save_all_pars = T, 
  save_model = "../models/m.exp1b.test.treat.woMT.vs.CNTL.numerically_coded_conditions.stan",
  file = "../models/exp1b.Test.treat.woMT.vs.CNTL.numerical_coded_conditions"
)
m.exp1b.test.treat.woST.vs.CNTL <- brm(
  IsCorrect ~ offset(individual_training_performance.offset) + Cond.treatTS.vs.CNTL + Cond.treatMT.vs.CNTL + 
    (1 + Cond.treatTS.vs.CNTL + Cond.treatMT.vs.CNTL + Cond.treatST.vs.CNTL | Sentence) + 
    (1 | WorkerID),  
  data = d.exp1b.test, family = bernoulli, 
  iter = nsamples.total, warmup = nsamples.warmup, 
  chains = max.cores, cores = max.cores,
  prior = my.newpriors.woST.vs.CNTL, save_all_pars = T, 
  save_model = "../models/m.exp1b.test.treat.woST.vs.CNTL.numerically_coded_conditions.stan",
  file = "../models/exp1b.Test.treat.woST.vs.CNTL.numerical_coded_conditions"
)

## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Step 5: Run the bridge sampler on all new models fit to the replication data
## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
filename.b1 = "../models/bridge_exp1b.Test.treat.numerical_coded_conditions.rds"
filename.b2.a = "../models/bridge_exp1b.Test.treat.woTS.vs.CNTL.numerical_coded_conditions.rds"
filename.b2.b = "../models/bridge_exp1b.Test.treat.woMT.vs.CNTL.numerical_coded_conditions.rds"
filename.b2.c = "../models/bridge_exp1b.Test.treat.woST.vs.CNTL.numerical_coded_conditions.rds"

if (!file.exists(filename.b1)) {
  b1 = bridge_sampler(
    samples = m.exp1b.test.treat.full, cores = 8, method = "warp3", maxiter = 1000)
  saveRDS(b1, file = filename.b1)
} else {
  b1 = readRDS(file = filename.b1)
}

if (!file.exists(filename.b2.a)) {
  b2.woTS.vs.CNTL = bridge_sampler(
    samples = m.exp1b.test.treat.woTS.vs.CNTL, cores = 8, method = "warp3", maxiter = 1000)
  saveRDS(b2.woTS.vs.CNTL, file = filename.b2.a)
} else {
  b2.woTS.vs.CNTL = readRDS(file = filename.b2.a)
}

if (!file.exists(filename.b2.b)) {
  b2.woMT.vs.CNTL = bridge_sampler(
    samples = m.exp1b.test.treat.woMT.vs.CNTL, cores = 8, method = "warp3", maxiter = 1000)
  saveRDS(b2.woMT.vs.CNTL, file = filename.b2.b)
} else {
  b2.woMT.vs.CNTL = readRDS(file = filename.b2.b)
}

if (!file.exists(filename.b2.c)) {
  b2.woST.vs.CNTL = bridge_sampler(
    samples = m.exp1b.test.treat.woST.vs.CNTL, cores = 8, method = "warp3", maxiter = 1000)
  saveRDS(b2.woST.vs.CNTL, file = filename.b2.c)
} else {
  b2.woST.vs.CNTL = readRDS(file = filename.b2.c)
}

## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Step 6: Calculate all the relevant Bayes Factors
## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
bf(b1, b2.woTS.vs.CNTL)
bf(b1, b2.woMT.vs.CNTL)
bf(b1, b2.woST.vs.CNTL)
```



## Questions 3

To test replication of Question 3, we are recoding all treatment contrasts so that the multi-talker condition is the baseline. This allows us to assess replication of the treatment effect of the single talker condition, compared to the multi-talker baseline.

```{r, replication analysis with treatment coding for Question 3}
## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Step 2: Run model on original data. But with numerically coded variables instead of contrast
## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
m.exp1a.test.treat.Q3 <- brm(
  IsCorrect ~ offset(individual_training_performance.linear) + 
    Cond.treatTS.vs.MT + Cond.treatST.vs.MT + Cond.treatCNTL.vs.MT + 
    (1 + Cond.treatTS.vs.MT + Cond.treatST.vs.MT + Cond.treatCNTL.vs.MT | Sentence) + 
    (1 | WorkerID),  
  data = d.exp1a.test, family = bernoulli, 
  iter = nsamples.total, warmup = nsamples.warmup, 
  chains = max.cores, cores = max.cores,
  prior = my.priors, 
  save_model = "../models/m.exp1a.test.treat.full.numerically_coded_conditions.Q3.stan", 
  file = "../models/exp1a.Test.treat.numerical_coded_conditions.Q3"
)

## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Step 3: Derive priors from original data.
## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
model = m.exp1a.test.treat.Q3
p = posterior_samples(model)

p %<>%
  dplyr::select(-starts_with("r_")) %>%
  dplyr::select(-starts_with("lp_")) %>%
  summarise_all(
    .funs = c("mean", "mode", "sd", "se") 
  ) %>%
  t() %>%
  as.data.frame() %>%
  rownames_to_column(var = "coef") %>%
  mutate(
    property = gsub("^.*_([a-z]+)$", "\\1", coef),
    coef = gsub("^(.*)_[a-z]+$", "\\1", coef)
  ) %>%
  spread(property, V1) %>%
  mutate(
    class = gsub("^([a-z]+)_.*$", "\\1", coef),
    coef = gsub("^[a-z]+_(.*)$", "\\1", coef),
    group = ifelse(gsub("^([A-Za-z]+)__.*$", "\\1", coef) %in% c("Sentence", "WorkerID"), gsub("^([A-Za-z]+)__.*$", "\\1", coef), ""),
    coef = gsub("^[A-Za-z]+__(.*)$", "\\1", coef),
    prior = case_when(
      class == "b" ~ paste0("normal(", mean, ",", sd, ")"),
      class == "sd" ~ paste0("normal(", mean, ",", sd, ")")
    )
  ) %>% 
  filter(class != "cor") %>%                          # For now removing correlation priors
  dplyr::select(prior, class, coef, group) %>%
  mutate(
    class = ifelse(coef == "Intercept" & class == "b", "Intercept", class),
    coef = ifelse(class == "Intercept", "", coef)
  )

my.newpriors.ST.vs.MT = c(
  set_prior(p[1,"prior"], class = p[1,"class"], coef = p[1,"coef"], group = p[1,"group"]),
  set_prior(p[2,"prior"], class = p[2,"class"], coef = p[2,"coef"], group = p[2,"group"]),
  set_prior(p[3,"prior"], class = p[3,"class"], coef = p[3,"coef"], group = p[3,"group"]),
  set_prior(p[4,"prior"], class = p[4,"class"], coef = p[4,"coef"], group = p[4,"group"]),
  set_prior(p[5,"prior"], class = p[5,"class"], coef = p[5,"coef"], group = p[5,"group"]),
  set_prior(p[6,"prior"], class = p[6,"class"], coef = p[6,"coef"], group = p[6,"group"]),
  set_prior(p[7,"prior"], class = p[7,"class"], coef = p[7,"coef"], group = p[7,"group"]),
  set_prior(p[8,"prior"], class = p[8,"class"], coef = p[8,"coef"], group = p[8,"group"]),
  set_prior(p[9,"prior"], class = p[9,"class"], coef = p[9,"coef"], group = p[9,"group"]),
  prior(lkj(1), class = cor)
)
my.newpriors.woST.vs.MT = my.newpriors.ST.vs.MT[-2,]

## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Step 4: Run same numerically-coded model on new data, but with priors based on original data  
## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
m.exp1b.test.treat.full.Q3 <- brm(
  IsCorrect ~ offset(individual_training_performance.linear) + 
    Cond.treatTS.vs.MT + Cond.treatST.vs.MT + Cond.treatCNTL.vs.MT + 
    (1 + Cond.treatTS.vs.MT + Cond.treatST.vs.MT + Cond.treatCNTL.vs.MT | Sentence) +
    (1 | WorkerID),  
  data = d.exp1b.test, family = bernoulli, 
  iter = nsamples.total, warmup = nsamples.warmup, 
  chains = max.cores, cores = max.cores,
  prior = my.newpriors.ST.vs.MT, save_all_pars = T, 
  save_model = "../models/m.exp1b.test.treat.full.numerically_coded_conditions.Q3.stan", 
  file = "../models/exp1b.Test.treat.numerical_coded_conditions.Q3"
)

## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Step 4: Run all three 'nullmodels' -- i.e. the same numerically-coded model on new data, but 
#         with one parameter set to 0 (removed). Again use priors based on original data. 
## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
m.exp1b.test.treat.woST.vs.MT <- brm(
  IsCorrect ~ offset(individual_training_performance.linear) + 
    Cond.treatTS.vs.MT + Cond.treatCNTL.vs.MT + 
    (1 + Cond.treatTS.vs.MT + Cond.treatST.vs.MT + Cond.treatCNTL.vs.MT | Sentence) + 
    (1 | WorkerID),  
  data = d.exp1b.test, family = bernoulli, 
  iter = nsamples.total, warmup = nsamples.warmup, 
  chains = max.cores, cores = max.cores,
  prior = my.newpriors.woST.vs.MT, save_all_pars = T, 
  save_model = "../models/m.exp1b.test.treat.woST.vs.MT.numerically_coded_conditions.stan",
  file = "../models/exp1b.Test.treat.woST.vs.MT.numerical_coded_conditions"
)

## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Step 5: Run the bridge sampler on all new models fit to the replication data
## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
filename.b1 = "../models/bridge_exp1b.Test.treat.numerical_coded_conditions.Q3.rds"
filename.b2.a = "../models/bridge_exp1b.Test.treat.woST.vs.MT.numerical_coded_conditions.rds"

if (!file.exists(filename.b1)) {
  b1 = bridge_sampler(
    samples = m.exp1b.test.treat.full.Q3, cores = 8, method = "warp3", maxiter = 1000)
  saveRDS(b1, file = filename.b1)
} else {
  b1 = readRDS(file = filename.b1)
}

if (!file.exists(filename.b2.a)) {
  b2.woST.vs.MT = bridge_sampler(
    samples = m.exp1b.test.treat.woST.vs.MT, cores = 8, method = "warp3", maxiter = 1000)
  saveRDS(b2.woST.vs.MT, file = filename.b2.a)
} else {
  b2.woST.vs.MT = readRDS(file = filename.b2.a)
}

## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Step 6: Calculate all the relevant Bayes Factors
## +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
bf(b1, b2.woST.vs.MT)
```


# Similarity-based analysis
```{r}
load("../data/Talker-similarity.RData")
```

One hypothesis is that listeners benefit from single talker exposure when the exposure and test talker are *similar* to each other. Experiment 1a and 1b both provide data on 20 unique combinations of exposure and test talkers in the single talker condition. It is thus theoretically possible to test whether similarity between the exposure and test talker predicts participants' transcription accuracy during test.

An adequate assessement of this possibility is beyond the scope of this paper. We did, however, explore two (overly simplistic) approaches to measuring similarity: a) subjective ratings of similarity obtained from an independent set of participants and b) the objective similarity between talkers in terms of their segmental phonetics. In both cases, our approach is best understood as a very coarse-grained approximation of the relevant quantity, based on what was feasible within the scope of this project. For example, our approximation of the objective similarity of talkers' phonetics is based on only their vowel productions (in addition to other limitations). 

Below we describe the two types of similarity measures, how they were obtained, and whether they predict talker-to-talker generalization in the single talker condition. To anticipate the results, neither of the two measures is predictive of transcription accuracy during test in the single talker condition. For our `objective similarity' measure, there is clear evidence that our approach fails to approximate the true objective similarity of the talkers' phonetics in any relevant way, making this test uninformative. For the subjective similarity ratings, the problem seems to be a different one. Our findings suggest that participants are able to judge the perceived similarity of talkers, and these judgments suggests that some of our exposure and test talkers are perceived as substantially more similar than others. However, we find that these subjective similarity ratings seem to primarily relu on basic suprasegmental speech properties, such as the talkers' fundamental frequency (F0). Since these properties are not *necessarily* predictive of how talkers realize segments and words, it is perhaps not surprising that this measure turns out not to reliably predict how much exposure to one talker facilitates accurate perception of another talker.

We nevertheless report our approach and findings here, primarily for two reasons. First, good reporting standard dictate that we report all tests we conducted. Second, we hope that the approaches explored here---flawed as they are---can be informative for other reseachers interested in similar questions.

## Subjective similarity ratings

We investigated whether perceived talker-to-talker similarity explains differences in cross-talker generalization. We obtained sentence-by-sentence similarity ratings for all exposure and test talker pairs from Exp 1 & 2 from 48 participants over Mechanical Turk. 


MORE INFORMATION HERE

### Methods

EXPLAIN ALSO HOW TALKER SCORE WAS OBTAINED.


### Summary of similarity scores
```{r prepare similarity analyses}
options(width = 1000)

# Prepare test data for join with similarity data
d.for_sim = d.exp1ab.test %>%
  filter(Condition2 %in% c("Single talker", "Talker-specific")) %>%
  mutate(
    TalkerPair = paste(
      gsub("^.*([0-9]{3})$", "\\1", TrainingTalkerID), 
      gsub("^.*([0-9]{3})$", "\\1", TestTalkerID), 
      sep = " "),
    TestTalker = gsub("^.*([0-9]{3})$", "\\1", TalkerPair)
  ) %>% 
  group_by(Experiment, Condition2, WorkerID, Sentence, TalkerPair, TestTalker, TrainingTestSet) %>%
  summarise(
    meanCorrect = mean(IsCorrect),
    NumSuccess = sum(IsCorrect),
    Num = length(IsCorrect),
    individual_training_performance.offset = mean(individual_training_performance.offset)
  ) 

# Join subjective similarity into test data
d.sim.subj = 
  d.for_sim %>%
  left_join(
    d.similarity.subjective %>%
      filter(RejectWorker == FALSE) %>%
      mutate(
        TalkerPair = gsub("^.*([0-9]{3}).*([0-9]{3})$", "\\1 \\2", TalkerPair),
        Set = gsub("^.*([0-9])$", "Test set \\1", Set)
      ) %>%
      group_by(TalkerPair, Sentence, Set) %>%
      summarise(Similarity = mean(Response))) %>%
  ungroup()
```

The following figure shows the mean similarity ratings and 95% bootstrapped confidence interval for each combination of exposure and test talkers. The six talkers to the right of the dividing line are the self-similarity ratings for the six talkers used during exposure---i.e., the similarity between exposure and test talker that participants in the talker-specific condition would experience. To the left of the dividing line are similarity ratings for all 20 combinations of exposure and test talker experienced by participants in the single talker condition. Unsurprisingly, acoustically identical sentences from the same talker were given the highest similarity ratings. While there was substantial variability in similarity ratings (reflected in the confidence intervals), the results also suggest that some of the exposure and test talkers were rated to be significantly more similar to each other than others. For example, the mean similarity ratings for the least similar talker pairs fall outside of the 95\% confidence intervals for the most similar talker pairs.

The second plot splits up these ratings by test talker and sentence set (which 16 of the 32 sentences were used during test, rather than exposure). Overall, the similarity ratings for pairs of exposure and test talker are relatively robust to which sentences (and thus recordings) were used during test: the ordering of talker combinations does not differ *much* between the first and second row of panels But there are are some instances in which the relative similarity of the exposure and test talker depends on the sentence. For example, the three least similar exposure talkers for test talker 043 seem to only differ in the relative similarity with regard to Set 2. Similarly, the second most similar exposure talker for test talker 032 either groups with the most similar or the next most similar exposure talker, depending on whether similarity was rated with regard to Set 1 or 2.

```{r subject similarity plots, warning=FALSE}
d.sim.subj %>%
  group_by(TalkerPair) %>%
  mutate(Similarity_order = mean(Similarity)) %>%
  ungroup() %>%
  mutate(TalkerPair = gsub(" ", "->", TalkerPair)) %>%
  arrange(Condition2, Similarity_order) %>%
  mutate(TalkerPair = factor(TalkerPair, levels = unique(TalkerPair))) %>%
  group_by(TalkerPair, TestTalker, Set, Sentence) %>%
  summarise(Similarity = mean(Similarity)) %>%
  na.omit() %>%
  dplyr::select(TalkerPair, TestTalker, Set, Sentence, Similarity) %>%
  distinct() %>%
  ggplot(aes(x = TalkerPair, y = Similarity)) +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange") +
  scale_x_discrete("Combination of exposure and test talker") +
  scale_y_continuous("Subjective similarity ratings", limits = c(1,7)) +
  geom_vline(xintercept = 20.5, linetype = 2, color = "darkgray") +
  theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggsave(filename = "../figures/similarity-subjective.pdf",
       width = 12, height = 3.5)

last_plot() +
  geom_vline(xintercept = 5.5, linetype = 2, color = "darkgray") +
  facet_grid(Set ~ TestTalker, scales = "free_x") 
ggsave(filename = "../figures/similarity-subjective-by test talker.pdf",
       width = 6, height = 7)
```

We do not know what exactly these similarity scores reflect---i.e., which properties of talkers' speech affected participants' perception of the sentence-to-sentence similarity. One potential concern is that explicit similarity judgments of the type used here are a form of meta-cognitive judgment, and that this type of judgment draws on different features than those relevant to the implicit (subconscious) processes underlying cross-talker generalization during speech perception. Specifically, it is possible---if not expected---that participants might rely largely on salient supra-segmental speech features, such as rythm and pitch. These feature are *not* necessarily reliable indicators of how similar two talkers are in their realization of sound categories. 

Indeed, there is some evidence that this is the case here. A multi-dimensional scaling analysis revealed two latent dimensions along which the six talkers differed. We extracted talker's mean pitch (F0) from the recordings used in the experiment, and found that the ordering of talkers' pitch closely correlates with the ordering of talkers on the first latent dimension:

![Multi-dimensional scaling (MDS) of similarity ratings. The first two latent dimensions derived from MDS are shown. The six talkers used (between-participants) during exposure in the single talker condition are shown in this space. For comparison the first dimension is plotted against the mean F0 of the six talkers (extracted from the recordings from each talker used during exposure).](../figures/multi-dimensional scaling.png)

This result has to be interpreted with caution, of course, since we only have six different talkers here. It does, however, provide *a priori* reason to expect the subjective similarity ratings to *not* be a good predictor of successful generalization from the exposure to the test talker. Even if listeners do use supra-segmental features to generalize experience from exposure to test, this generalization will only facilitate listeners' performance during test to the extent that the exposure and test talker are indeed similar in how they realize sound categories and words.

### Predicting talker-to-talker generalization in the single talker condition

```{r subject similarity analysis}
run_subjective_similarity_analysis = function(
  d, 
  experiment = c("1a", "1b"), 
  condition = "Single talker",
  filename,
  # should the data be aggregated at the level of unique combinations of
  # talker pair and set?
  aggregate = TRUE
) {
  my.priors = c(
    prior(student_t(3,0,2.5), class = b),
    prior(cauchy(0,5), class = sd)
  )
  
  m = brm(
    NumSuccess | trials(Num) ~ 
      offset(individual_training_performance.offset) + Similarity + 
      (1 | Sentence) + (1 | WorkerID),  
    data = d %>%
      filter(
        Experiment %in% experiment,
        Condition2 == condition) %>%
        { if (aggregate)
          group_by(., TalkerPair, Set) %>%
          mutate(Similarity = mean(Similarity)) %>%
          ungroup() else . } %>%
      mutate(Similarity = my_scale(Similarity)), 
    family = binomial, cores = max.cores, chains = 4, 
    warmup = 2000, iter = 4000, 
    prior = my.priors, file = paste0("../models/", filename))
  
  return(m)
}

m.exp1a.subjective_similarity.ST = run_subjective_similarity_analysis(
  d.sim.subj, 
  experiment = "1a",
  filename = "exp1a.subjective_similarity.ST")

m.exp1b.subjective_similarity.ST = run_subjective_similarity_analysis(
  d.sim.subj, 
  experiment = "1b",
  filename = "exp1b.subjective_similarity.ST")
```


To analyze whether subjective similarity between the exposure and test talker predicts the accuracy during test, we calculated the mean similarity rating for each unique combination of the 20 different pairs of exposure and test talker, and the two different training & test sets. 

We then fit a Bayesian mixed-effect logistic regression to just the test data from the single talker condition. Separate analyses were performed for Experiments 1a and 1b. We predict sentence-level accuracy during test from the mean subjective similarity scores for each exposure-test talker combination (centered and scaled by twice its standard deviations, following Gelman, 2008). Following the main analysis, the model contained an offset term, correcting for individual differences that were already present during the exposure phase. Also following the main analysis, the model contained the maximum random effect structure---in this case, random intercepts by item and by participant.

For Experiment 1a, there is positive support for the the hypothesis that the subjective similarity between the exposure and test talker predicts the accuracy during test:

```{r}
summary(m.exp1a.subjective_similarity.ST)
xhypothesis(hypothesis(m.exp1a.subjective_similarity.ST, "Similarity > 0"))

p = plot(conditional_effects(m.exp1a.subjective_similarity.ST), plot = FALSE) 
p[[1]] + ylim(c(.7, 1)) + theme_bw()
```

For Experiment 1b, however, the opposite relationship was observed:

```{r}
summary(m.exp1b.subjective_similarity.ST)
xhypothesis(hypothesis(m.exp1b.subjective_similarity.ST, "Similarity > 0"))

p = plot(conditional_effects(m.exp1b.subjective_similarity.ST), plot = FALSE) 
p[[1]] + ylim(c(.7, 1)) + theme_bw()
```

One potential reason for the weak / absent relation between the subjective similarity ratings and performance during test might be that the similarity ratings are strongly influenced by properties of the speech signal that are irrelevant for talker-to-talker generalization. The multi-dimensional scaling analysis mentioned above suggests as much, in that it shows that similarity ratings depend strongly on the talkers' mean F0. Such supra-segmental properties are expected to be somewhat predictive of *some* talker-specific phonetics (e.g., vowel realization), but are not expected to be predictive of other talker-specific phonetics. 

Other possible nuisance influence on the subjective similarity ratings are incidental differences between recordings, such as overt disfluencies (such as "uh", "um", or restarts) or speech errors (i.e., a pronunciation that is atypical for that talker). Recall that participants in the rating study rated the similarity between pairs of recordings of the same sentence. If one talker's recording contains an overt disfluency, and the other other talker's recording does not, this might lead participants to rate these two productions as less similar---but this type of dissimilarity is unlikely to be predictive of talker-to-talker generalization. 

We note that similarity ratings *are* predictive of performance during test if similarity ratings are *not* averaged for each unique combination of exposure talker, test talker, and training & test sets---both for Experiment 1a and 1b: 

```{r}
m.exp1a.subjective_similarity.ST.sentence_level = run_subjective_similarity_analysis(
  d.sim.subj, 
  experiment = "1a",
  filename = "exp1a.subjective_similarity.ST.sentence_level", 
  aggregate = FALSE)

m.exp1b.subjective_similarity.ST.sentence_level = run_subjective_similarity_analysis(
  d.sim.subj, 
  experiment = "1b",
  filename = "exp1b.subjective_similarity.ST.sentence_level", 
  aggregate = FALSE)

xhypothesis(hypothesis(m.exp1a.subjective_similarity.ST.sentence_level, "Similarity > 0"))
xhypothesis(hypothesis(m.exp1b.subjective_similarity.ST.sentence_level, "Similarity > 0"))
```

If similarity ratings---which are available for each unique pair of recordings of the same sentence---are instead entered into the mixed-effects logistic regression as-is---i.e., while allowing to vary between sentences---we find very strong support for the hypothesis that subjective similarity predicts talker-to-talker generalization (BFs > 150). However, this might simply reflect that a) raters gave lower similarity scores to pairs of sentences when one sentence differed from the other in terms of overt disfluences or speech errors, and b) the presence of overt disfluencies or speech errors also makes it harder to correctly transcribe a sentence. In that case, sentence-level similarity would correlate with sentence-level transcription accuracy for reasons independet of successful talker-to-talker transfer. Given that we the talker-level similarity ratings to not be consistently predictive of performance during test, we believe this to be the most likely explanation for the very strong correlation between sentence-level similarity ratings and performance during test.



## Objective similarity in vowel statistics
```{r preparation for objective similarity plots}
# Merge IO-based ('objective') similarity information with test data
d.sim.obj = 
  d.for_sim %>%
  left_join(
    d.similarity.objective %>%
      mutate(
        modelType = plyr::mapvalues(
          modelType,
          c("ST", "TS"),
          c("Single talker", "Talker-specific")
        ),
        Set = gsub("^.*([0-9])$", "Test set \\1", TrainingTestSet)
      ) %>%
      filter(
        modelType %in% c("Single talker", "Talker-specific"),
        Condition == "Non-control",
        cues == "Lobanov Normalized F1xF2"
      ) %>%
      mutate(
        TalkerPair = gsub("^.*([0-9]{3}).*([0-9]{3})$", "\\1 \\2", paste(Talker_train, Talker_test))
      ) %>%
      group_by(TalkerPair, Sentence, Set, modelType) %>%
      summarise_at(vars(lhood, posterior, posterior_choice, accuracy),
                   .funs = mean)) %>% 
  ungroup()
```

MORE INFORMATION HERE. INCL. DOWNSIDES / LIMTATIONS OF THIS APPROACH

Our sole motivation in chosing *vowels*, rather than all (or other) types of segments, is feasibility. The center of vowels (on which we measured F1 and F2) are comparatively easy to identify and segment. The cues relevant to vowel identification in American English (primarily F1 and F2) are comparatively well understood (Hillenbrand et al., 1995 and follow-up studies), and easily be extracted from the segmented speech. 

That said, the approach taken here is limited in many ways. MORE HERE. LACKING SOUNDS, NO WORD AND CONTEXT EFFECTS, NO SUPRA-SEGMENTAL ... 

### Methods

MORE INFO HERE

### Summary of ideal observer-derived accuracies

The following plots show the predicted accuracy of an ideal observer applied to the vowel's of the test talker, based on training on the vowels of the exposure talker. The six talkers to the right of the dividing line correspond to the talker-specific condition. To the left of the dividing line are predicted accuracies for all 20 combinations of exposure and test talker in the single talker condition. What is striking is that accuracy is *not* always highest in the talker-specific condition, despite the fact that in this case the exposure stimuli that the model is trained on come from the same talker as the test stimuli that the model is tested on. This provides strong evidence that the models are overfit to the training data, presumably because we simply do not have sufficiently many instances of the different vowels in the training data. In line with this hypothesis, the relative ordering of the different single talker models differs quite a bit depending on which set of sentences was used for training and which set was used for test (see second plot).

This strongly suggests that **the measure of objective similarity we have developed here is a *bad* approximation of the true objective similarity of the vowel statistics.** We therefore would *not* expect this measure to be predictive of transcription accuracy during test.

```{r objective similarity plots}
d.sim.obj %>%
  group_by(TalkerPair) %>%
  mutate(accuracy_order = mean(accuracy)) %>%
  ungroup() %>%
  mutate(TalkerPair = gsub(" ", "->", TalkerPair)) %>%
  arrange(modelType, accuracy_order) %>%
  mutate(TalkerPair = factor(TalkerPair, levels = unique(TalkerPair))) %>%
  na.omit() %>%
  dplyr::select(TalkerPair, TestTalker, Set, Sentence, accuracy) %>%
  distinct() %>%
  ggplot(aes(x = TalkerPair, y = accuracy)) +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange") +
  scale_x_discrete("Combination of exposure and test talker") +
  scale_y_continuous("Predicted accuracy\non test talker's vowels", limits = c(0,1)) +
  geom_vline(xintercept = 20.5, linetype = 2, color = "darkgray") +
  theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggsave(filename = "../figures/similarity-objective.pdf",
       width = 12, height = 3.5)

last_plot() +
  geom_vline(xintercept = 5.5, linetype = 2, color = "darkgray") +
  scale_y_continuous("Predicted accuracy on test talker's vowels", limits = c(0,1)) +
  facet_grid(Set ~ TestTalker, scales = "free_x") 
ggsave(filename = "../figures/similarity-objective-by test talker.pdf",
       width = 6, height = 7)
```

### Predicting talker-to-talker generalization in the single talker condition
Following the same Bayesian mixed-effects regression approach that we applied to the analysis of subjective similarity scores, we predicted sentence-level accuracy during test in the single talker condition from the objective similarity scores. Following the main analysis, the model contained an offset term, correcting for individual differences that were already present during the exposure phase. Also following the main analysis, the model contained the maximum random effect structure---in this case, random intercepts by item and by participant. We calculated each participants' objective similarity score---i.e., the average ideal observer-predicted accuracy during test--- and transformed it into logits (to use the same units as the outcome we are aiming to predict: transcription accuracy). We then centered and scaled this predictor by twice its standard deviations (following Gelman, 2008). 

```{r objective similarity analysis}
run_objective_similarity_analysis = function(
  d, 
  experiment = c("1a", "1b"), 
  condition = "Single talker",
  filename,
  # should the data be aggregated at the level of unique combinations of
  # talker pair and set?
  aggregate = TRUE
) {
  my.priors = c(
    prior(student_t(3,0,2.5), class = b),
    prior(cauchy(0,5), class = sd)
  )
  
  m = brm(
    NumSuccess | trials(Num) ~ 
      offset(individual_training_performance.offset) + accuracy_logit + 
      (1 | Sentence) + (1 | WorkerID),  
    data = d %>%
      filter(
        Experiment %in% experiment,
        Condition2 == condition) %>%
      { if (aggregate)
          group_by(., TalkerPair, Set) %>%
          mutate(accuracy = mean(accuracy)) %>%
          ungroup() else . } %>%
      mutate(accuracy_logit = my_scale(qlogis(accuracy))), 
    family = binomial, cores = max.cores, chains = 4, 
    warmup = 2000, iter = 4000, 
    prior = my.priors, file = paste0("../models/", filename))
  
  return(m)
}

m.exp1a.objective_similarity.ST = run_objective_similarity_analysis(
  d.sim.obj, 
  experiment = "1a",
  filename = "exp1a.objective_similarity.ST")

m.exp1b.objective_similarity.ST = run_objective_similarity_analysis(
  d.sim.obj, 
  experiment = "1b",
  filename = "exp1b.objective_similarity.ST")
```

For Experiment 1a, there is very little support for the the hypothesis that the ideal observer predicted accuracy on the test talker's vowels predicts the actual accuracy of human listeners during test:

```{r}
xhypothesis(hypothesis(m.exp1a.objective_similarity.ST, "accuracy_logit > 0"))
```

The same holds for Experiment 1b:

```{r}
xhypothesis(hypothesis(m.exp1b.objective_similarity.ST, "accuracy_logit > 0"))
```








# Pilot experiment
```{r, pilot compute a priori perceptual ability and join into data frames}
# Compute indiviudal training performance
d.pilot.training = d.pilot %>%
  filter(PartOfExp == "training")
d.pilot.test = d.pilot %>%
  filter(PartOfExp == "test")

d.pilot.training$Quintile = as.numeric(cut(d.pilot.training$Trial, breaks = 5, labels = seq(1,5))) - 1

# Converged- this is the model stored as models/d.pilot.Training_IndividualDifferences.rds
if (file.exists("../models/pilot.Training_IndividualDifferences.rds")) {
  m.pilot.train = readRDS("../models/pilot.Training_IndividualDifferences.rds")
} else {
  m.pilot.train <- glmer(
    IsCorrect ~ Condition2 + Quintile +
      (1 + Condition2 | Sentence) + (1 | WorkerID) + (1 | CurrentTalkerID),
    data = d.pilot.training, family = binomial)
  saveRDS(m.pilot.train, file = "../models/pilot.Training_IndividualDifferences.rds")
}

d.pilot.predframe_TRAIN <- data.frame(WorkerID=levels(d.pilot.training$WorkerID))

## Take the intercept of SF model and add random effects from CON model to indicate individual values for each worker in CON condition
dd <- ranef(m.pilot.train)[["WorkerID"]]
dd <- cbind(WorkerID = rownames(dd), dd)
rownames(dd) <- NULL

d.pilot.predframe_TRAIN <- left_join(d.pilot.predframe_TRAIN, dd, by = "WorkerID")
d.pilot.predframe_TRAIN$Intercept.ranef <- d.pilot.predframe_TRAIN$`(Intercept)`
d.pilot.predframe_TRAIN$`(Intercept)` <- NULL
d.pilot.training <- left_join(d.pilot.training, d.pilot.predframe_TRAIN)
d.pilot.test <- left_join(d.pilot.test, d.pilot.predframe_TRAIN)
```

```{r, pilot test block analysis - main level}
contrasts(d.pilot.test$Condition2) = contr.sum(2) * -1
colnames(contrasts(d.pilot.test$Condition2)) = ("TS.vs.CNTL")

m.pilot.test <- brm(
  IsCorrect ~ Condition2 + Intercept.ranef + 
    (1 + Condition2 | Sentence) + (1 | WorkerID),  
  data = d.pilot.test, family = bernoulli, cores = max.cores, file = "../models/pilot.Test")

# The model including an interaction term was not significantly different than the one without. 
# m.pilot.test.interaction <- glmer(IsCorrect ~ Condition.sum * Intercept.ranef + 
#     (1 + Condition.sum| Sentence) + (1 | WorkerID),  
#     data = d.pilot.test, family = bernoulli, cores = max.cores)

myGplot.defaults(type = "paper")
p.test.proportion %+% 
  (d.pilot %>%
  group_by(WorkerID, Sentence) %>%
  distinct(.keep_all = TRUE) %>%
  filter(PartOfExp == "test") %>% 
  group_by(WorkerID, Condition2) %>%
	dplyr::summarise(PropKeywordsCorrect = mean(PropKeywordsCorrect)))
ggsave(filename = "../figures/p.pilot.ts.cntl.pdf", device = cairo_pdf, width = 3, height = 4, dpi=300)

p = p.test.proportion %+% 
  (d = d.pilot %>%
  group_by(WorkerID, Sentence) %>%
  distinct(.keep_all = TRUE) %>%
  filter(PartOfExp == "test") %>% 
  group_by(WorkerID, Condition2, CurrentTalkerID) %>%
	dplyr::summarise(PropKeywordsCorrect = mean(PropKeywordsCorrect)))

p +
  geom_text(
    data = d %>%
      group_by(Condition2, CurrentTalkerID) %>%
      dplyr::summarise(PropKeywordsCorrect = mean(PropKeywordsCorrect)) %>%
      group_by(CurrentTalkerID) %>%
      dplyr::summarise(
        gain = round((PropKeywordsCorrect[2] - PropKeywordsCorrect[1]) * 100, 1),
        text = paste0(ifelse(gain > 0, "+", ""), gain, "%")),
    x = 1.5,
    y = .3,
    aes(label = text),
    inherit.aes = F
  ) +
  facet_wrap(~ CurrentTalkerID, nrow = 1) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggsave(filename = "../figures/p.pilot.ts.cntl_byTalker.pdf", device = cairo_pdf, width = 8, height = 4.5, dpi=300)
```

```{r, pilot test block analysis - model output, size = "footnotesize"}
summary(m.pilot.test)
```

## Hypothesis testing
```{r, pilot test block analysis - hypotheses}
xhypotheses(
  list(
    hypothesis(m.pilot.test, "Condition2TS.vs.CNTL > 0", class = "b"),
    hypothesis(m.pilot.test, "Intercept.ranef > 0", class = "b")))
```











# Session info
```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
